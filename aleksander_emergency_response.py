#!/usr/bin/env python3
"""
Aleksander's Emergency Response Protocol
=======================================
Technical Lead Emergency Analysis System
"""

import json
import sys
from datetime import datetime
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path("/Users/artur/coursor-agents-destiny-folder")
sys.path.insert(0, str(PROJECT_ROOT))

from agents.core.aleksander_agent import AleksanderAgent
from agents.technical.helena_agent import HelenaAgent
from agents.technical.viktor_agent import ViktorAgent

class EmergencyTechnicalResponse:
    def __init__(self):
        self.timestamp = datetime.now()
        self.project_root = PROJECT_ROOT
        
    def analyze_postgres_crisis(self):
        """Coordinate emergency response to PostgreSQL performance crisis"""
        
        print("=" * 80)
        print("ðŸš¨ ALEKSANDER: EMERGENCY TECHNICAL RESPONSE ACTIVATED")
        print("=" * 80)
        print(f"Time: {self.timestamp}")
        print("Issue: PostgreSQL Performance Crisis - 431+ second hang")
        print()
        
        # Read the analysis
        analysis_path = self.project_root / "POSTGRES_STUCK_ANALYSIS.md"
        with open(analysis_path, 'r') as f:
            analysis = f.read()
        
        print("ðŸ“Š TECHNICAL ANALYSIS SUMMARY:")
        print("-" * 40)
        
        # Extract key issues
        issues = {
            "root_causes": [
                "Heavy GIN index overhead (3 GIN indexes on JSONB/arrays)",
                "180+ concurrent SQL files (712KB) being processed",
                "Real-time processing creating continuous write load",
                "Lock contention on metadata queries"
            ],
            "immediate_risks": [
                "Complete database lockup possible",
                "Data loss if forced shutdown required",
                "Cascading failures in dependent systems",
                "User operations completely blocked"
            ],
            "affected_systems": [
                "es_document_references table",
                "Real-time processor pipeline",
                "Helena's automated workflows",
                "All database-dependent operations"
            ]
        }
        
        print("\nðŸ”´ CRITICAL ISSUES IDENTIFIED:")
        for cause in issues["root_causes"]:
            print(f"  â€¢ {cause}")
            
        print("\nâš ï¸  IMMEDIATE RISKS:")
        for risk in issues["immediate_risks"]:
            print(f"  â€¢ {risk}")
            
        # Generate action plan
        action_plan = self.generate_emergency_action_plan()
        
        # Write technical response
        response = {
            "timestamp": self.timestamp.isoformat(),
            "responder": "Aleksander Ciesielski (Tech Lead)",
            "severity": "CRITICAL",
            "analysis": issues,
            "action_plan": action_plan,
            "implementation_order": [
                "1. IMMEDIATE: Kill long-running queries",
                "2. IMMEDIATE: Disable real-time processors",
                "3. SHORT-TERM: Implement connection pooling",
                "4. MID-TERM: Redesign index strategy",
                "5. LONG-TERM: Separate read/write workloads"
            ]
        }
        
        # Save response
        response_path = self.project_root / "TECHNICAL_EMERGENCY_RESPONSE.json"
        with open(response_path, 'w') as f:
            json.dump(response, f, indent=2)
            
        # Create immediate fix script
        self.create_emergency_fix_script()
        
        print("\nâœ… EMERGENCY RESPONSE PREPARED")
        print(f"ðŸ“„ Response saved to: {response_path}")
        print("ðŸ”§ Emergency fix script created: emergency_postgres_fix.sh")
        print()
        print("ðŸŽ¯ IMMEDIATE ACTIONS REQUIRED:")
        for i, action in enumerate(action_plan["immediate"], 1):
            print(f"   {i}. {action['action']}")
        
        return response
        
    def generate_emergency_action_plan(self):
        """Generate comprehensive action plan"""
        
        return {
            "immediate": [
                {
                    "action": "Kill all queries running > 5 minutes",
                    "command": "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = 'destiny_team' AND state = 'active' AND query_start < NOW() - interval '5 minutes';",
                    "risk": "May interrupt legitimate long operations"
                },
                {
                    "action": "Disable real-time processors temporarily",
                    "command": "pkill -f 'helena_realtime_processor|morning_brief'",
                    "risk": "Will pause automated workflows"
                },
                {
                    "action": "Drop and recreate problematic GIN indexes with fastupdate=off",
                    "command": "ALTER INDEX idx_es_doc_refs_tags SET (fastupdate = off);",
                    "risk": "Brief performance impact during rebuild"
                }
            ],
            "short_term": [
                {
                    "action": "Implement pgbouncer connection pooling",
                    "timeline": "2-4 hours",
                    "benefit": "Reduce connection overhead by 70%"
                },
                {
                    "action": "Batch real-time inserts (5-second windows)",
                    "timeline": "4-6 hours", 
                    "benefit": "Reduce write frequency by 95%"
                },
                {
                    "action": "Implement query timeout policies",
                    "timeline": "1 hour",
                    "benefit": "Prevent runaway queries"
                }
            ],
            "long_term": [
                {
                    "action": "Separate OLTP and OLAP workloads",
                    "timeline": "1-2 weeks",
                    "benefit": "Eliminate read/write contention"
                },
                {
                    "action": "Implement TimescaleDB for time-series data",
                    "timeline": "2-3 weeks",
                    "benefit": "Optimize for append-only workloads"
                },
                {
                    "action": "Design proper data archival strategy",
                    "timeline": "3-4 weeks",
                    "benefit": "Reduce active dataset size"
                }
            ]
        }
        
    def create_emergency_fix_script(self):
        """Create emergency fix script"""
        
        script_content = """#!/bin/bash
# PostgreSQL Emergency Fix Script
# Generated by: Aleksander Ciesielski
# Purpose: Resolve critical database performance issues

echo "ðŸš¨ PostgreSQL Emergency Fix Protocol"
echo "===================================="
echo ""

# Step 1: Show current connections and queries
echo "ðŸ“Š Current database state:"
psql -h localhost -U user -d destiny_team -c "
SELECT pid, usename, application_name, state, query_start, 
       LEFT(query, 60) as query_preview
FROM pg_stat_activity 
WHERE datname = 'destiny_team' 
ORDER BY query_start;"

# Step 2: Kill long-running queries (with confirmation)
echo ""
echo "ðŸ”ª Terminating queries running > 5 minutes..."
read -p "Proceed? (y/n) " -n 1 -r
echo ""
if [[ $REPLY =~ ^[Yy]$ ]]; then
    psql -h localhost -U user -d destiny_team -c "
    SELECT pg_terminate_backend(pid), pid, usename, 
           NOW() - query_start as duration
    FROM pg_stat_activity 
    WHERE datname = 'destiny_team' 
    AND state = 'active' 
    AND query_start < NOW() - interval '5 minutes';"
fi

# Step 3: Stop real-time processors
echo ""
echo "â¸ï¸  Pausing real-time processors..."
pkill -f 'helena_realtime_processor' || true
pkill -f 'morning_brief' || true
pkill -f 'realtime_md_watcher' || true

# Step 4: Optimize GIN indexes
echo ""
echo "ðŸ”§ Optimizing GIN indexes..."
psql -h localhost -U user -d destiny_team << EOF
-- Disable fast update for GIN indexes
ALTER INDEX idx_es_doc_refs_tags SET (fastupdate = off);
ALTER INDEX idx_es_doc_refs_metadata SET (fastupdate = off);

-- Force pending list cleanup
SELECT gin_clean_pending_list('idx_es_doc_refs_tags');
SELECT gin_clean_pending_list('idx_es_doc_refs_metadata');

-- Update statistics
ANALYZE es_document_references;
EOF

# Step 5: Check table bloat
echo ""
echo "ðŸ“ Checking table bloat..."
psql -h localhost -U user -d destiny_team -c "
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size
FROM pg_tables 
WHERE tablename = 'es_document_references';"

echo ""
echo "âœ… Emergency fixes applied!"
echo ""
echo "ðŸ“‹ Next steps:"
echo "1. Monitor database performance"
echo "2. Implement connection pooling"  
echo "3. Redesign batch processing"
echo ""
"""
        
        script_path = self.project_root / "emergency_postgres_fix.sh"
        with open(script_path, 'w') as f:
            f.write(script_content)
        
        # Make executable
        import os
        os.chmod(script_path, 0o755)
        
        return script_path

def main():
    """Execute emergency response"""
    responder = EmergencyTechnicalResponse()
    response = responder.analyze_postgres_crisis()
    
    # Notify team
    print("\nðŸ“¢ TEAM NOTIFICATION:")
    print("-" * 40)
    print("Helena: Check your real-time processors")
    print("Viktor: Review system resource usage")
    print("System Admin: Prepare for emergency maintenance")
    print()
    print("ðŸ”´ STATUS: CRITICAL - Immediate action required")
    
if __name__ == "__main__":
    main()