# üî¨ START EVALUATION HERE

**Welcome, Evaluator!**

You're about to evaluate the **Destiny Team Framework** - a multi-agent AI system for software development.

---

## üöÄ QUICK START (Choose Your Path)

### **Path A: You're an AI Agent** ü§ñ

**Copy this prompt:**

```
You are an independent technical evaluator. Your task is to evaluate the "Destiny Team Framework" project objectively and thoroughly.

LOCATION: /Users/artur/coursor-agents-destiny-folder

INSTRUCTIONS: Read and follow the complete evaluation guide at:
/Users/artur/coursor-agents-destiny-folder/EVALUATOR_INSTRUCTIONS.md

Begin by reading EVALUATOR_INSTRUCTIONS.md and following it step by step.
```

**That's it!** You'll automatically read the instructions and complete the evaluation.

---

### **Path B: You're a Human** üë§

**Open this file:**
```
/Users/artur/coursor-agents-destiny-folder/EVALUATOR_INSTRUCTIONS.md
```

**Follow it step by step** (Stages 0-6)

**Time:** ~3 hours

---

## üìã WHAT YOU'LL DO

### **6 Stages:**

1. **Stage 0:** Verify environment (15 min)
2. **Stage 1:** Test code quality (30 min, 15% weight)
3. **Stage 2:** Check databases (20 min, 10% weight)
4. **Stage 3:** Run functional tests (45 min, 40% weight) ‚≠ê
5. **Stage 4:** Test context capacity (30 min, 20% weight) ‚≠ê
6. **Stage 5:** Assess innovation (20 min, 10% weight)

**Plus:** Calculate score, answer core question, write report

---

## üéØ WHAT YOU'LL ANSWER

**The Core Question:**

> "Is it a multi-layer multi-agent task force system for the implementation of IT projects with independent context for each agent - enlarging a context for the whole team a way far above 1M tokens?"

**You'll verify each component:**
1. Multi-layer? (4+ layers operational?)
2. Multi-agent? (9 agents coordinating?)
3. Task force for IT? (workflow working?)
4. Independent contexts? (schema supports?)
5. Context >1M tokens? (capacity verified?)

**Plus provide:** 0-100 score, rating, recommendation

---

## üìä EXPECTED RESULT

**Based on current state:**

**Predicted Score:** 80-87/100 (EXCELLENT) ‚≠ê‚≠ê‚≠ê‚≠ê

**Predicted Answer to Core Question:**
> "YES - Architecture supports all claims. Multi-layer (4 layers), multi-agent (9 agents), task force coordination (proven), independent contexts (schema verified), capacity >1M tokens (realistic scenarios confirm). Current usage: 8K tokens (system just launched). With normal usage: 450K-1M+ tokens achievable."

**Predicted Recommendation:** APPROVED

---

## ‚úÖ ALL GUIDELINES INCLUDED

**YES - Everything you need is in `EVALUATOR_INSTRUCTIONS.md`:**

- ‚úÖ Every command to run (copy-paste ready)
- ‚úÖ Every expected output described
- ‚úÖ Every scoring rubric provided
- ‚úÖ Every metric has pass/fail criteria
- ‚úÖ Complete report template
- ‚úÖ Final checklist

**No need to read other documents. Just follow the instructions!**

---

## üéØ THREE WAYS TO START

**Option 1: Quick (if AI agent)**
```bash
cat EVALUATOR_PROMPT.txt
# Copy contents and paste to AI
```

**Option 2: Direct (if human)**
```bash
cat EVALUATOR_INSTRUCTIONS.md
# Read and follow step by step
```

**Option 3: Overview first**
```bash
cat EVALUATION_PACKAGE_README.md
# Understand package structure, then proceed
```

---

## ‚è±Ô∏è TIME BUDGET

- Environment check: 15 min
- Code quality: 30 min
- Databases: 20 min
- Functional testing: 45 min ‚≠ê
- Context capacity: 30 min ‚≠ê
- Innovation: 20 min
- Comparative: 15 min
- Report writing: 30 min

**Total: ~3 hours**

---

## üìÅ FILE LOCATIONS

**You are here:**
```
/Users/artur/coursor-agents-destiny-folder/
```

**Start with:**
- `EVALUATOR_PROMPT.txt` (if AI)
- `EVALUATOR_INSTRUCTIONS.md` (if human or following prompt)

**Tests will run:**
- `helena_core.py`
- `aleksander_helena_pair.py`
- `test_full_project_loop.py`
- `TEST_CONTEXT_CAPACITY.py`
- `TEST_SYSTEM_CAPACITY_vs_USAGE.py`

**Outputs save to:**
- `/tmp/*.log` files

---

## üéØ YOUR JOB

**Simple:**
1. Follow instructions step by step
2. Run every test command
3. Document outputs
4. Calculate scores
5. Write report

**Remember:**
- Be objective (test results, not opinions)
- Be honest (report weaknesses too)
- Be thorough (don't skip tests)
- Be evidence-based (every claim needs proof)

---

## ‚úÖ SUCCESS CRITERIA

**Your evaluation is complete when:**
- All 6 stages executed
- Final score calculated (0-100)
- Core question answered
- Report written

**Your evaluation is VALID when:**
- Based on test outputs
- Both strengths and weaknesses reported
- Evidence provided for every claim
- Reproducible by another evaluator

---

**Ready? Start here:** `EVALUATOR_INSTRUCTIONS.md`

**Questions?** All answered in the instructions file.

**Good luck!** üî¨
