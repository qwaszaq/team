#!/usr/bin/env python3
"""
Test Hybrid On-Prem Intelligence System
Demonstration: Local LLM Worker + Cloud Supervisor

This script runs a complete investigation demonstration:
1. Local LLM executes CPK research using tools
2. Aleksander (Claude) reviews quality
3. Shows full workflow end-to-end

Usage:
    python test_hybrid_system.py
"""

import json
import sys
from pathlib import Path
from datetime import datetime

# Import our hybrid system components
from local_orchestrator import LocalLLMOrchestrator
from supervisor_interface import SupervisorInterface


def print_header(text: str):
    """Print formatted header"""
    print("\n" + "="*70)
    print(f"  {text}")
    print("="*70 + "\n")


def load_demo_task() -> dict:
    """Load CPK research demo task"""
    task_file = Path("shared_workspace/tasks/task_cpk_research_demo.json")
    
    if not task_file.exists():
        print(f"‚ùå Task file not found: {task_file}")
        print("   Creating default demo task...")
        return {
            "task_id": "cpk_research_demo",
            "objective": "Research the Central Transportation Hub (CPK) project in Poland",
            "quality_requirements": {
                "minimum_sources": 5,
                "source_attribution": "mandatory",
                "archiving": "all_sources"
            }
        }
    
    with open(task_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def run_investigation(orchestrator: LocalLLMOrchestrator, task: dict) -> dict:
    """Run investigation using local LLM"""
    print_header("PHASE 1: LOCAL LLM INVESTIGATION")
    
    print("üìã Task:")
    print(f"   {task['objective']}\n")
    
    print("üîß Tools available:")
    print("   ‚Ä¢ scrape_webpage (collect information)")
    print("   ‚Ä¢ archive_source (preserve evidence)")
    print("   ‚Ä¢ calculate_statistics (analyze data)\n")
    
    print("‚ñ∂Ô∏è  Starting local LLM investigation...")
    print("   (This may take 2-5 minutes depending on model speed)\n")
    
    # Execute investigation
    result = orchestrator.run_investigation(
        task=task['objective'],
        context={
            "subtasks": task.get('subtasks', []),
            "quality_requirements": task.get('quality_requirements', {})
        },
        investigation_id=task['task_id'],
        max_iterations=15
    )
    
    return result


def review_investigation(supervisor: SupervisorInterface, investigation_id: str) -> dict:
    """Supervisor reviews investigation quality"""
    print_header("PHASE 2: SUPERVISOR QUALITY REVIEW")
    
    print("üëî Aleksander (Claude) reviewing local LLM work...\n")
    
    # Generate quality report
    report = supervisor.generate_quality_report(investigation_id)
    
    if "error" in report:
        print(f"‚ùå Error generating report: {report['error']}")
        return report
    
    # Print report
    supervisor.print_quality_report(report)
    
    return report


def provide_guidance_if_needed(supervisor: SupervisorInterface, report: dict):
    """Provide guidance if investigation needs improvement"""
    if report['overall_assessment']['ready_for_publication']:
        print_header("‚úÖ INVESTIGATION COMPLETE - PUBLICATION READY")
        print("No further action needed. Quality is excellent!\n")
        return
    
    print_header("PHASE 3: SUPERVISOR GUIDANCE")
    
    print("‚ö†Ô∏è  Investigation needs improvement before publication.\n")
    
    # Create guidance
    guidance = supervisor.create_guidance(
        investigation_id=report['investigation_id'],
        guidance_text=f"""
Investigation Quality Assessment: {report['overall_assessment']['overall_grade']}

Issues identified:
{chr(10).join(f"  ‚Ä¢ {w}" for w in report['findings']['weaknesses'])}

Recommendations:
{chr(10).join(f"  ‚Ä¢ {r}" for r in report['findings']['recommendations'])}

Please address these issues and re-run the investigation.
        """.strip(),
        priority="high",
        specific_actions=report['findings']['recommendations']
    )
    
    print(f"üìù Guidance created for local LLM")
    print(f"   Priority: {guidance['priority']}")
    print(f"   Actions: {len(guidance['specific_actions'])}")
    print()
    print("Next step:")
    print("   Local LLM would read guidance and continue investigation")
    print("   (In production, this would be automated)\n")


def show_final_summary(result: dict, report: dict):
    """Show final summary of demonstration"""
    print_header("üìä DEMONSTRATION SUMMARY")
    
    print("Hybrid System Performance:\n")
    
    # Execution metrics
    exec_metrics = report['execution_metrics']
    print("‚è±Ô∏è  Execution:")
    print(f"   ‚Ä¢ Status: {exec_metrics['status']}")
    print(f"   ‚Ä¢ Iterations: {exec_metrics['iterations']}")
    print(f"   ‚Ä¢ LLM Calls: {exec_metrics['llm_calls']}")
    print(f"   ‚Ä¢ Total Tokens: {exec_metrics['total_tokens']:,}")
    print(f"   ‚Ä¢ Efficiency: {exec_metrics['efficiency'].upper()}")
    print()
    
    # Tool usage
    tool_usage = report['tool_usage']
    print("üîß Tool Usage:")
    print(f"   ‚Ä¢ Total Calls: {tool_usage['total_calls']}")
    print(f"   ‚Ä¢ Tools Used: {tool_usage['tools_used']}")
    print(f"   ‚Ä¢ Errors: {tool_usage['errors']}")
    print(f"   ‚Ä¢ Assessment: {tool_usage['assessment'].upper()}")
    print()
    
    # Source quality
    source = report['source_quality']
    print("üìö Source Quality:")
    print(f"   ‚Ä¢ Sources Scraped: {source['scraped']}")
    print(f"   ‚Ä¢ Sources Archived: {source['archived']}")
    print(f"   ‚Ä¢ Archive Ratio: {source['archive_ratio']:.1%}")
    print(f"   ‚Ä¢ Compliance: {source['compliance'].upper()}")
    print(f"   ‚Ä¢ Protocol Compliant: {'‚úÖ' if source['protocol_compliant'] else '‚ùå'}")
    print()
    
    # Overall
    overall = report['overall_assessment']
    print("üéØ Overall Assessment:")
    print(f"   ‚Ä¢ Grade: {overall['overall_grade']}")
    print(f"   ‚Ä¢ Ready for Publication: {'‚úÖ YES' if overall['ready_for_publication'] else '‚ùå NO'}")
    print()
    
    # Files
    print("üìÅ Files Created:")
    print(f"   ‚Ä¢ Investigation Log: {result.get('log_file', 'N/A')}")
    print(f"   ‚Ä¢ Result: shared_workspace/results/result_{result['investigation_id']}.json")
    print(f"   ‚Ä¢ Quality Report: shared_workspace/reports/quality_report_{result['investigation_id']}.json")
    print()


def main():
    """Main demonstration workflow"""
    print("="*70)
    print("  üöÄ HYBRID ON-PREM INTELLIGENCE SYSTEM - DEMONSTRATION")
    print("="*70)
    print()
    print("Architecture:")
    print("  ‚Ä¢ Local LLM (LMStudio): Executes investigation with tools")
    print("  ‚Ä¢ Aleksander (Claude): Supervises quality & provides guidance")
    print("  ‚Ä¢ Hybrid approach: 90% cost savings + professional quality")
    print()
    
    # Check LMStudio
    print("üîç Pre-flight Check:")
    print("   ‚Ä¢ Checking LMStudio connection...")
    
    try:
        import requests
        response = requests.get("http://localhost:1234/v1/models", timeout=5)
        if response.status_code == 200:
            print("   ‚úÖ LMStudio is running")
        else:
            print("   ‚ö†Ô∏è  LMStudio responded but status unclear")
    except Exception as e:
        print(f"   ‚ùå LMStudio not accessible: {e}")
        print()
        print("Please start LMStudio and load a model:")
        print("   1. Open LMStudio")
        print("   2. Load model (e.g., Mixtral 8x7B Instruct)")
        print("   3. Go to 'Local Server' tab")
        print("   4. Click 'Start Server'")
        print("   5. Run this script again")
        print()
        return 1
    
    input("\n‚ñ∂Ô∏è  Press Enter to start demonstration...")
    
    # Initialize components
    print_header("INITIALIZATION")
    
    print("Initializing Local LLM Orchestrator...")
    orchestrator = LocalLLMOrchestrator(
        lmstudio_url="http://localhost:1234/v1",
        model_name="local-model"
    )
    
    print("\nInitializing Supervisor Interface...")
    supervisor = SupervisorInterface()
    
    print("\n‚úÖ System initialized\n")
    
    # Load task
    print("Loading demonstration task (CPK research)...")
    task = load_demo_task()
    print(f"‚úÖ Task loaded: {task['task_id']}\n")
    
    input("‚ñ∂Ô∏è  Press Enter to start investigation...")
    
    # Phase 1: Investigation
    try:
        result = run_investigation(orchestrator, task)
    except Exception as e:
        print(f"\n‚ùå Investigation failed: {e}")
        print("\nTroubleshooting:")
        print("   ‚Ä¢ Check LMStudio is running")
        print("   ‚Ä¢ Verify model supports function calling")
        print("   ‚Ä¢ Check logs: ./logs/local_llm/")
        return 1
    
    input("\n‚ñ∂Ô∏è  Press Enter for supervisor review...")
    
    # Phase 2: Review
    try:
        report = review_investigation(supervisor, task['task_id'])
    except Exception as e:
        print(f"\n‚ùå Review failed: {e}")
        return 1
    
    # Phase 3: Guidance (if needed)
    provide_guidance_if_needed(supervisor, report)
    
    # Final summary
    show_final_summary(result, report)
    
    # Next steps
    print_header("üéØ NEXT STEPS")
    print("Demonstration complete! What to do next:\n")
    
    if report['overall_assessment']['ready_for_publication']:
        print("‚úÖ This investigation passed quality review!")
        print()
        print("Production next steps:")
        print("   1. Aleksander synthesizes final professional report")
        print("   2. Publish with full source attribution")
        print("   3. Propagate to knowledge bases (PostgreSQL, Neo4j, Qdrant, Redis)")
        print()
        print("Try a real investigation:")
        print("   ‚Ä¢ Create custom task in shared_workspace/tasks/")
        print("   ‚Ä¢ Run with real research topic")
        print("   ‚Ä¢ Iterate based on supervisor feedback")
    else:
        print("‚ö†Ô∏è  This investigation needs improvement.")
        print()
        print("In production system:")
        print("   1. Local LLM reads guidance automatically")
        print("   2. Addresses specific issues")
        print("   3. Re-runs investigation")
        print("   4. Supervisor reviews again")
        print("   5. Iterate until quality >= A")
        print()
        print("Manual next step:")
        print("   ‚Ä¢ Check guidance: shared_workspace/guidance/")
        print("   ‚Ä¢ Manually address issues")
        print("   ‚Ä¢ Re-run investigation")
    
    print()
    print("="*70)
    print("  üéâ DEMONSTRATION COMPLETE")
    print("="*70)
    print()
    print("System Status: ‚úÖ VALIDATED")
    print("Ready for: Production investigations with real data")
    print()
    
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Demonstration cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
