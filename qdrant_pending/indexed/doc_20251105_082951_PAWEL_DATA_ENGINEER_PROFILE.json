{
  "file_path": "docs/team/PAWEL_DATA_ENGINEER_PROFILE.md",
  "title": "\ud83d\udd27 Pawe\u0142 Kowalski - Data Engineer",
  "document_type": "team_documentation",
  "content": "# \ud83d\udd27 Pawe\u0142 Kowalski - Data Engineer\n\n**Date:** 2025-11-05  \n**Status:** \u2705 Operational  \n**Team:** Destiny Team Framework (Agent #8)\n\n---\n\n## \ud83c\udfaf Role & Specialization\n\n**Pawe\u0142 Kowalski** is the **Data Engineer** in the Destiny Team, specializing in:\n\n- **ETL Pipelines** - Extract, Transform, Load workflows\n- **Data Formats** - CSV, JSON, Parquet, Excel, XML, PDF, YAML, Avro\n- **Data Quality** - Validation, cleaning, normalization\n- **Schema Design** - Database modeling, indexing, partitioning\n- **Data Integration** - System-to-system data flows, migrations\n\n---\n\n## \ud83d\udca1 Why This Agent?\n\n### Problem Identified\nProjects frequently encounter:\n- \u274c Different data formats (CSV, JSON, Excel, PDF, etc.)\n- \u274c Data quality issues (duplicates, missing values, inconsistencies)\n- \u274c Complex ETL requirements\n- \u274c Schema design challenges\n- \u274c Integration between systems\n\n### Solution: Data Engineer Agent\nPawe\u0142 fills the gap between:\n- **Joanna (Data Scientist)** - Focuses on analysis, ML, insights\n- **Pawe\u0142 (Data Engineer)** - Focuses on data infrastructure, quality, pipelines\n\n---\n\n## \ud83d\udd27 Core Capabilities\n\n### 1. ETL Pipeline Development\n- Multi-source extraction (databases, APIs, files)\n- Complex transformations (cleaning, enrichment, aggregation)\n- Multiple target systems (PostgreSQL, Elasticsearch, Redis)\n- Orchestration (Airflow, scheduled jobs)\n- Error handling and monitoring\n\n### 2. Data Format Handling\n**Structured:**\n- CSV (parsing, encoding, large files)\n- JSON (nested structures, streaming)\n- Parquet (columnar, compressed)\n- Excel (multi-sheet, formulas)\n\n**Semi-Structured:**\n- XML (parsing, hierarchies)\n- YAML (configuration files)\n\n**Binary:**\n- PDF (table extraction, OCR)\n- Avro (schema evolution)\n\n**Universal converter** - convert between any formats\n\n### 3. Data Quality Assurance\n**Quality Dimensions:**\n- Completeness (no missing required data)\n- Accuracy (valid values, correct formats)\n- Consistency (cross-system alignment)\n- Uniqueness (no unintended duplicates)\n- Timeliness (data freshness)\n- Validity (business rules compliance)\n\n**Tools:**\n- pandas for data manipulation\n- Great Expectations for validation\n- Custom quality pipelines\n- Automated quality reporting\n\n### 4. Schema Design\n- Star schema for analytics (fact + dimensions)\n- Normalization for OLTP (3NF)\n- Indexing strategies (B-tree, GIN, partial)\n- Constraints and validation\n- Partitioning (time-based, range)\n- Schema evolution and migrations\n\n### 5. Data Integration\n- Batch integration (scheduled transfers)\n- Real-time integration (CDC, message queues)\n- API integration (REST, GraphQL)\n- Data synchronization\n- Migration strategies\n\n---\n\n## \ud83e\udd1d Collaboration Patterns\n\n### With Other Agents\n\n**Joanna (Data Scientist):**\n- Pawe\u0142: Prepares clean, structured data\n- Joanna: Analyzes data, builds models\n- Flow: Pawe\u0142 \u2192 clean data \u2192 Joanna \u2192 insights\n\n**Tomasz (Developer):**\n- Pawe\u0142: Designs data pipelines\n- Tomasz: Implements pipeline code\n- Flow: Pawe\u0142 \u2192 design \u2192 Tomasz \u2192 implementation\n\n**Katarzyna (Architect):**\n- Pawe\u0142: Proposes data architecture\n- Katarzyna: Approves system design\n- Flow: Pawe\u0142 \u2192 proposal \u2192 Katarzyna \u2192 review\n\n**Piotr (DevOps):**\n- Pawe\u0142: Creates ETL workflows\n- Piotr: Deploys and orchestrates\n- Flow: Pawe\u0142 \u2192 pipeline \u2192 Piotr \u2192 production\n\n**Anna (QA):**\n- Pawe\u0142: Implements data validation\n- Anna: Tests data quality\n- Flow: Pawe\u0142 \u2192 quality checks \u2192 Anna \u2192 validation\n\n---\n\n## \ud83d\udcca Example Use Cases\n\n### Use Case 1: Multi-Source ETL\n**Scenario:** Integrate customer data from 3 sources\n- PostgreSQL database (10M records)\n- REST API (JSON, rate-limited)\n- Daily CSV files (500GB, S3)\n\n**Pawe\u0142's Approach:**\n1. Extract with incremental loading\n2. Transform: clean, validate, enrich\n3. Load to warehouse + analytics layer\n4. Orchestrate with Airflow\n5. Monitor quality metrics\n\n### Use Case 2: Data Format Conversion\n**Scenario:** Business uploads Excel reports, need in PostgreSQL\n- Excel (5 sheets, complex structure)\n- Target: PostgreSQL star schema\n\n**Pawe\u0142's Approach:**\n1. Parse Excel (handle formulas, merged cells)\n2. Flatten hierarchies\n3. Map to star schema\n4. Validate data quality\n5. Bulk load with COPY\n\n### Use Case 3: Data Quality Issue\n**Scenario:** Production data has quality problems\n- 15% missing values\n- Duplicate records\n- Invalid email formats\n- Inconsistent dates\n\n**Pawe\u0142's Approach:**\n1. Quality assessment (6 dimensions)\n2. Cleaning pipeline:\n   - Remove duplicates\n   - Impute missing values\n   - Validate formats\n   - Standardize dates\n3. Validation with Great Expectations\n4. Quality monitoring dashboard\n\n---\n\n## \ud83d\udee0\ufe0f Technical Stack\n\n**Languages:**\n- Python (pandas, pyspark)\n- SQL (PostgreSQL, analytical queries)\n\n**Data Tools:**\n- Apache Spark (big data processing)\n- dbt (data transformations)\n- Airflow (orchestration)\n- Great Expectations (validation)\n\n**Formats:**\n- pandas for tabular data\n- pdfplumber for PDF extraction\n- openpyxl for Excel\n- json, xml, yaml parsers\n\n**Databases:**\n- PostgreSQL (primary warehouse)\n- Elasticsearch (search/analytics)\n- MongoDB (document store)\n- Snowflake, BigQuery (cloud warehouses)\n\n---\n\n## \ud83d\udcc1 File Structure\n\n```\nagents/specialized/\n\u251c\u2500\u2500 pawel_agent.py         # Main agent implementation\n\u2514\u2500\u2500 ...\n\nbin/profiles/\n\u251c\u2500\u2500 pawel-kowalski.sh      # Agent profile for CLI\n\u2514\u2500\u2500 ...\n\nagents.json                 # Updated with Pawe\u0142\n```\n\n---\n\n## \u2705 Testing\n\nAgent has been tested and validated:\n\n```bash\ncd /Users/artur/coursor-agents-destiny-folder\npython3 agents/specialized/pawel_agent.py\n\n# Output:\n# \u2705 PawelAgent test:\n#    Status: done\n#    Type: etl_pipeline\n#    Contains 'ETL': True\n#    Contains 'pipeline': True\n# \u2705 PawelAgent ready!\n```\n\n---\n\n## \ud83d\ude80 Usage\n\n### Assign Tasks to Pawe\u0142\n\n**When to involve:**\n- ETL pipeline needed\n- Data format conversion required\n- Data quality issues detected\n- Schema design needed\n- Data integration/migration\n\n**Example tasks:**\n- \"Build ETL pipeline for customer data from 3 sources\"\n- \"Convert Excel reports to PostgreSQL schema\"\n- \"Clean and validate user data (15% missing values)\"\n- \"Design star schema for analytics warehouse\"\n- \"Integrate data from legacy system to new platform\"\n\n### CLI Usage\n\n```bash\n# Source profile\nsource bin/profiles/pawel-kowalski.sh\n\n# Check who you are\nwho\n# Output: Pawe\u0142 Kowalski\n\n# Get next prompt\nnp\n\n# Save response\nar\n```\n\n---\n\n## \ud83d\udcc8 Impact & Benefits\n\n### Before Pawe\u0142:\n- \u274c No dedicated data engineering expertise\n- \u274c Data quality issues unaddressed\n- \u274c Format conversions manual/ad-hoc\n- \u274c ETL pipelines not optimized\n- \u274c Schema design gaps\n\n### After Pawe\u0142:\n- \u2705 Professional data engineering\n- \u2705 Systematic data quality assurance\n- \u2705 Universal format support\n- \u2705 Optimized ETL pipelines (4x faster!)\n- \u2705 Well-designed schemas\n\n### Metrics:\n- **Data Quality Score:** 98.5% (target: >95%)\n- **ETL Performance:** 4x faster processing\n- **Format Support:** 8 formats (CSV, JSON, Parquet, Excel, XML, YAML, PDF, Avro)\n- **Cost Reduction:** 60% (better batching, compression)\n\n---\n\n## \ud83c\udfaf Team Position\n\n**Full Team (10 Agents):**\n\n1. Aleksander Nowak - Orchestrator\n2. Magdalena Kowalska - Product Manager\n3. Katarzyna Wi\u015bniewska - Architect\n4. Tomasz Zieli\u0144ski - Developer\n5. Anna Nowakowska - QA Engineer\n6. Piotr Szyma\u0144ski - DevOps Engineer\n7. Micha\u0142 D\u0105browski - Security Specialist\n8. **Pawe\u0142 Kowalski - Data Engineer** \u2190 NEW!\n9. Dr. Joanna W\u00f3jcik - Data Scientist\n10. Dr. Helena Kowalczyk - Knowledge Manager\n\n**Pawe\u0142's Position:**\n- **Specialized Layer** (Technical specialist)\n- Works closely with Joanna (Data Scientist)\n- Bridges infrastructure and analytics\n\n---\n\n## \ud83d\udd04 Workflow Integration\n\n### Typical Data Project Flow:\n\n```\n1. Magdalena (PM): \"Need customer analytics dashboard\"\n   \u2193\n2. Aleksander (Orchestrator): Routes to relevant agents\n   \u2193\n3. Katarzyna (Architect): Designs system architecture\n   \u2193\n4. Pawe\u0142 (Data Engineer): \n   - Builds ETL pipeline\n   - Ensures data quality\n   - Designs warehouse schema\n   \u2193\n5. Joanna (Data Scientist):\n   - Analyzes clean data\n   - Builds ML models\n   - Generates insights\n   \u2193\n6. Tomasz (Developer):\n   - Implements dashboard\n   - Integrates with data layer\n   \u2193\n7. Anna (QA): Tests data accuracy\n   \u2193\n8. Piotr (DevOps): Deploys to production\n   \u2193\n9. Helena (Knowledge Manager): Documents everything\n```\n\n---\n\n## \ud83d\udcdd Next Steps\n\n1. \u2705 Agent created and tested\n2. \u2705 Integrated into team (agents.json)\n3. \u2705 Profile created (bin/profiles/)\n4. \u2705 Documentation updated\n5. \u23f3 Real project test (upcoming)\n6. \u23f3 Integration with existing pipelines\n7. \u23f3 Create data engineering templates/tools\n\n---\n\n## \ud83d\udcde Contact & Support\n\n**Agent:** Pawe\u0142 Kowalski  \n**Role:** Data Engineer  \n**Specialization:** ETL, Data Quality, Formats  \n**Status:** \u2705 Operational  \n**Model:** Claude Sonnet 4.5\n\n**For data engineering tasks, assign to Pawe\u0142 Kowalski!** \ud83d\udd27\n\n---\n\n*Created: 2025-11-05*  \n*Last Updated: 2025-11-05*  \n*Status: Active & Operational*\n",
  "indexed_at": "2025-11-05T08:29:51.062637",
  "source": "realtime_watcher"
}