{
  "file_path": "docs/technical/AGENT_TOOLKITS_COMPLETE.md",
  "title": "\ud83d\udee0\ufe0f Agent Toolkits - Complete Technical Specification",
  "document_type": "team_documentation",
  "content": "# \ud83d\udee0\ufe0f Agent Toolkits - Complete Technical Specification\n\n**Prepared by:** Alex Morgan (Technical Liaison) + Elena Volkov (OSINT)  \n**Date:** 2025-11-04  \n**Status:** \ud83d\udd28 IMPLEMENTATION READY  \n**Focus:** Practical tools agenci mog\u0105 u\u017cywa\u0107 natychmiast  \n\n---\n\n## \ud83c\udfaf Overview\n\nKa\u017cdy agent potrzebuje **zestawu narz\u0119dzi** do swojej pracy. Ten dokument definiuje:\n1. **Scraping Toolkit** - zbieranie danych z internetu\n2. **Mathematical Toolkit** - obliczenia, analiza, statystyka\n3. **Image Intelligence Toolkit** - analiza obraz\u00f3w\n4. **Text Intelligence Toolkit** - analiza tekstu\n5. **Geolocation Toolkit** - lokalizacja geograficzna\n\n---\n\n## \ud83d\udce6 1. SCRAPING TOOLKIT\n\n### **Purpose:** Zbieranie danych z websites, APIs, social media\n\n### **Tech Stack:**\n\n```python\n# requirements.txt\nrequests==2.31.0          # HTTP requests (podstawa)\nbeautifulsoup4==4.12.2    # HTML parsing (proste)\nlxml==4.9.3               # XML/HTML parser (szybki)\nscrapy==2.11.0            # Full scraping framework (zaawansowane)\nselenium==4.15.0          # Browser automation (JavaScript)\nplaywright==1.40.0        # Modern browser automation (lepszy ni\u017c Selenium)\nhttpx==0.25.0             # Async HTTP client\naiohttp==3.9.0            # Async HTTP dla Scrapy\nfake-useragent==1.4.0     # Losowe User-Agents (unikanie blokady)\n```\n\n### **A. Basic Web Scraping (BeautifulSoup)**\n\n```python\n\"\"\"\nUse case: Proste website'y, HTML parsing\nBest for: Statyczne strony, pojedyncze requesty\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom fake_useragent import UserAgent\n\nclass BasicScraper:\n    \"\"\"Elena's basic scraping tool\"\"\"\n    \n    def __init__(self):\n        self.session = requests.Session()\n        self.ua = UserAgent()\n        \n    def fetch_page(self, url, timeout=10):\n        \"\"\"Pobierz stron\u0119 HTML\"\"\"\n        headers = {\n            'User-Agent': self.ua.random,\n            'Accept': 'text/html,application/xhtml+xml',\n            'Accept-Language': 'en-US,en;q=0.9',\n        }\n        \n        try:\n            response = self.session.get(url, headers=headers, timeout=timeout)\n            response.raise_for_status()\n            return response.text\n        except requests.RequestException as e:\n            print(f\"Error fetching {url}: {e}\")\n            return None\n    \n    def parse_html(self, html):\n        \"\"\"Parse HTML do BeautifulSoup object\"\"\"\n        return BeautifulSoup(html, 'lxml')\n    \n    def extract_links(self, soup, base_url=None):\n        \"\"\"Wyci\u0105gnij wszystkie linki\"\"\"\n        links = []\n        for a in soup.find_all('a', href=True):\n            href = a['href']\n            if base_url and not href.startswith('http'):\n                from urllib.parse import urljoin\n                href = urljoin(base_url, href)\n            links.append({\n                'url': href,\n                'text': a.get_text(strip=True)\n            })\n        return links\n    \n    def extract_text(self, soup):\n        \"\"\"Wyci\u0105gnij ca\u0142y tekst ze strony\"\"\"\n        # Remove script and style elements\n        for script in soup(['script', 'style']):\n            script.decompose()\n        \n        text = soup.get_text(separator=' ', strip=True)\n        return text\n    \n    def extract_metadata(self, soup):\n        \"\"\"Wyci\u0105gnij meta tags\"\"\"\n        metadata = {}\n        \n        # Title\n        if soup.title:\n            metadata['title'] = soup.title.string\n        \n        # Meta tags\n        for meta in soup.find_all('meta'):\n            name = meta.get('name') or meta.get('property')\n            content = meta.get('content')\n            if name and content:\n                metadata[name] = content\n        \n        return metadata\n    \n    def extract_tables(self, soup):\n        \"\"\"Wyci\u0105gnij tabele jako lista dict\u00f3w\"\"\"\n        tables = []\n        \n        for table in soup.find_all('table'):\n            rows = []\n            headers = []\n            \n            # Extract headers\n            header_row = table.find('thead') or table.find('tr')\n            if header_row:\n                headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]\n            \n            # Extract data rows\n            for tr in table.find_all('tr')[1:]:  # Skip header row\n                cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]\n                if cells:\n                    if headers and len(cells) == len(headers):\n                        rows.append(dict(zip(headers, cells)))\n                    else:\n                        rows.append(cells)\n            \n            tables.append({\n                'headers': headers,\n                'rows': rows\n            })\n        \n        return tables\n\n# Example usage:\nscraper = BasicScraper()\nhtml = scraper.fetch_page('https://example.com')\nsoup = scraper.parse_html(html)\nlinks = scraper.extract_links(soup)\ntext = scraper.extract_text(soup)\ntables = scraper.extract_tables(soup)\n```\n\n### **B. Dynamic Content Scraping (Playwright)**\n\n```python\n\"\"\"\nUse case: Websites z JavaScript, dynamic loading, SPAs\nBest for: Modern websites, social media, interactive pages\n\"\"\"\n\nfrom playwright.sync_api import sync_playwright\nimport time\n\nclass DynamicScraper:\n    \"\"\"Scraping websites z JavaScript\"\"\"\n    \n    def __init__(self, headless=True):\n        self.headless = headless\n        self.playwright = None\n        self.browser = None\n        self.context = None\n    \n    def start(self):\n        \"\"\"Start browser\"\"\"\n        self.playwright = sync_playwright().start()\n        self.browser = self.playwright.chromium.launch(\n            headless=self.headless,\n            args=['--disable-blink-features=AutomationControlled']\n        )\n        self.context = self.browser.new_context(\n            viewport={'width': 1920, 'height': 1080},\n            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n        )\n    \n    def stop(self):\n        \"\"\"Close browser\"\"\"\n        if self.context:\n            self.context.close()\n        if self.browser:\n            self.browser.close()\n        if self.playwright:\n            self.playwright.stop()\n    \n    def scrape_page(self, url, wait_for=None, scroll=False):\n        \"\"\"\n        Scrape dynamic page\n        \n        Args:\n            url: URL to scrape\n            wait_for: CSS selector to wait for (optional)\n            scroll: Scroll to bottom to load lazy content\n        \"\"\"\n        page = self.context.new_page()\n        \n        try:\n            # Navigate\n            page.goto(url, wait_until='networkidle')\n            \n            # Wait for specific element\n            if wait_for:\n                page.wait_for_selector(wait_for, timeout=10000)\n            \n            # Scroll if needed (lazy loading)\n            if scroll:\n                self._scroll_to_bottom(page)\n            \n            # Get content\n            content = page.content()\n            \n            return content\n            \n        finally:\n            page.close()\n    \n    def _scroll_to_bottom(self, page):\n        \"\"\"Scroll to bottom to trigger lazy loading\"\"\"\n        previous_height = page.evaluate('document.body.scrollHeight')\n        \n        while True:\n            # Scroll down\n            page.evaluate('window.scrollTo(0, document.body.scrollHeight)')\n            time.sleep(1)\n            \n            # Check if new content loaded\n            new_height = page.evaluate('document.body.scrollHeight')\n            if new_height == previous_height:\n                break\n            previous_height = new_height\n    \n    def screenshot(self, url, output_path):\n        \"\"\"Take screenshot\"\"\"\n        page = self.context.new_page()\n        try:\n            page.goto(url, wait_until='networkidle')\n            page.screenshot(path=output_path, full_page=True)\n        finally:\n            page.close()\n    \n    def extract_with_js(self, url, js_code):\n        \"\"\"Execute JavaScript to extract data\"\"\"\n        page = self.context.new_page()\n        try:\n            page.goto(url, wait_until='networkidle')\n            result = page.evaluate(js_code)\n            return result\n        finally:\n            page.close()\n\n# Example usage:\nscraper = DynamicScraper()\nscraper.start()\n\n# Scrape dynamic content\nhtml = scraper.scrape_page('https://twitter.com/search?q=osint', \n                           wait_for='article',\n                           scroll=True)\n\n# Take screenshot\nscraper.screenshot('https://example.com', 'screenshot.png')\n\n# Extract with custom JS\ntweets = scraper.extract_with_js('https://twitter.com/...',\n    '''\n    Array.from(document.querySelectorAll('article')).map(article => ({\n        text: article.innerText,\n        time: article.querySelector('time')?.dateTime\n    }))\n    ''')\n\nscraper.stop()\n```\n\n### **C. Large-Scale Scraping (Scrapy)**\n\n```python\n\"\"\"\nUse case: Scraping ca\u0142ych websites, tysi\u0105ce stron\nBest for: Systematic scraping, following links, sitemap crawling\n\"\"\"\n\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass UniversalSpider(scrapy.Spider):\n    \"\"\"Universal spider for any website\"\"\"\n    \n    name = 'universal'\n    \n    custom_settings = {\n        'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)',\n        'ROBOTSTXT_OBEY': True,  # Respect robots.txt\n        'CONCURRENT_REQUESTS': 8,\n        'DOWNLOAD_DELAY': 1,  # Be nice: 1 second between requests\n        'AUTOTHROTTLE_ENABLED': True,\n    }\n    \n    def __init__(self, start_url, allowed_domains=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start_urls = [start_url]\n        if allowed_domains:\n            self.allowed_domains = allowed_domains\n    \n    def parse(self, response):\n        \"\"\"Parse page and extract data\"\"\"\n        \n        # Extract data\n        yield {\n            'url': response.url,\n            'title': response.css('title::text').get(),\n            'text': ' '.join(response.css('p::text').getall()),\n            'links': response.css('a::attr(href)').getall(),\n            'images': response.css('img::attr(src)').getall(),\n        }\n        \n        # Follow links\n        for link in response.css('a::attr(href)').getall():\n            if link:\n                yield response.follow(link, callback=self.parse)\n\n# Example: Scrape entire website\ndef scrape_website(start_url, output_file='output.json'):\n    \"\"\"Scrape website and save to JSON\"\"\"\n    \n    process = CrawlerProcess(settings={\n        'FEEDS': {\n            output_file: {'format': 'json'},\n        },\n    })\n    \n    process.crawl(UniversalSpider, start_url=start_url)\n    process.start()\n\n# Usage:\n# scrape_website('https://example.com', 'example_data.json')\n```\n\n### **D. API Client (Structured)**\n\n```python\n\"\"\"\nUse case: APIs (Twitter, Reddit, etc.)\nBest for: Structured data, rate-limited access\n\"\"\"\n\nimport requests\nfrom typing import Dict, List\nimport time\nfrom datetime import datetime\n\nclass APIClient:\n    \"\"\"Generic API client with rate limiting\"\"\"\n    \n    def __init__(self, base_url, api_key=None, rate_limit=10):\n        \"\"\"\n        Args:\n            base_url: Base URL for API\n            api_key: API key (if needed)\n            rate_limit: Max requests per second\n        \"\"\"\n        self.base_url = base_url\n        self.api_key = api_key\n        self.rate_limit = rate_limit\n        self.last_request_time = 0\n        \n        self.session = requests.Session()\n        if api_key:\n            self.session.headers['Authorization'] = f'Bearer {api_key}'\n    \n    def _rate_limit_wait(self):\n        \"\"\"Wait if needed to respect rate limit\"\"\"\n        if self.rate_limit:\n            time_since_last = time.time() - self.last_request_time\n            wait_time = (1.0 / self.rate_limit) - time_since_last\n            if wait_time > 0:\n                time.sleep(wait_time)\n        self.last_request_time = time.time()\n    \n    def get(self, endpoint, params=None):\n        \"\"\"GET request\"\"\"\n        self._rate_limit_wait()\n        \n        url = f\"{self.base_url}/{endpoint}\"\n        response = self.session.get(url, params=params)\n        response.raise_for_status()\n        return response.json()\n    \n    def post(self, endpoint, data=None, json=None):\n        \"\"\"POST request\"\"\"\n        self._rate_limit_wait()\n        \n        url = f\"{self.base_url}/{endpoint}\"\n        response = self.session.post(url, data=data, json=json)\n        response.raise_for_status()\n        return response.json()\n    \n    def paginated_get(self, endpoint, params=None, limit=100):\n        \"\"\"Get all results from paginated endpoint\"\"\"\n        results = []\n        page = 1\n        \n        while True:\n            page_params = params.copy() if params else {}\n            page_params['page'] = page\n            page_params['per_page'] = 100\n            \n            data = self.get(endpoint, params=page_params)\n            \n            if not data or len(data) == 0:\n                break\n            \n            results.extend(data)\n            \n            if len(results) >= limit:\n                break\n            \n            page += 1\n        \n        return results[:limit]\n\n# Example: Sejm API client (from our previous work!)\nclass SejmAPIClient(APIClient):\n    \"\"\"Client for Sejm API\"\"\"\n    \n    def __init__(self):\n        super().__init__(base_url='https://api.sejm.gov.pl')\n    \n    def get_committees(self, term=9):\n        \"\"\"Get all committees\"\"\"\n        return self.get(f'sejm/term{term}/committees')\n    \n    def get_committee_sittings(self, term, code):\n        \"\"\"Get committee sittings\"\"\"\n        return self.get(f'sejm/term{term}/committees/{code}/sittings')\n```\n\n---\n\n## \ud83e\uddee 2. MATHEMATICAL TOOLKIT\n\n### **Purpose:** Obliczenia, statystyka, analiza danych\n\n### **Tech Stack:**\n\n```python\n# requirements.txt\nnumpy==1.24.3             # Arrays, linear algebra\nscipy==1.11.3             # Scientific computing\npandas==2.1.1             # DataFrames, data analysis\nscikit-learn==1.3.1       # Machine learning\nstatsmodels==0.14.0       # Statistical models\nmatplotlib==3.8.0         # Plotting\nseaborn==0.12.2           # Statistical visualization\nplotly==5.17.0            # Interactive plots\n```\n\n### **A. NumPy - Array Operations**\n\n```python\n\"\"\"\nUse case: Numerical calculations, arrays, linear algebra\nBest for: Fast mathematical operations\n\"\"\"\n\nimport numpy as np\n\nclass MathToolkit:\n    \"\"\"Mathematical operations for Maya (Data Analyst)\"\"\"\n    \n    @staticmethod\n    def basic_stats(data):\n        \"\"\"Calculate basic statistics\"\"\"\n        arr = np.array(data)\n        \n        return {\n            'mean': np.mean(arr),\n            'median': np.median(arr),\n            'std': np.std(arr),\n            'min': np.min(arr),\n            'max': np.max(arr),\n            'quartiles': np.percentile(arr, [25, 50, 75]),\n            'count': len(arr)\n        }\n    \n    @staticmethod\n    def correlation(x, y):\n        \"\"\"Calculate correlation between two variables\"\"\"\n        return np.corrcoef(x, y)[0, 1]\n    \n    @staticmethod\n    def moving_average(data, window=3):\n        \"\"\"Calculate moving average\"\"\"\n        return np.convolve(data, np.ones(window)/window, mode='valid')\n    \n    @staticmethod\n    def normalize(data):\n        \"\"\"Normalize data to 0-1 range\"\"\"\n        arr = np.array(data)\n        min_val = np.min(arr)\n        max_val = np.max(arr)\n        return (arr - min_val) / (max_val - min_val)\n    \n    @staticmethod\n    def detect_outliers(data, threshold=3):\n        \"\"\"Detect outliers using z-score\"\"\"\n        arr = np.array(data)\n        z_scores = np.abs((arr - np.mean(arr)) / np.std(arr))\n        return np.where(z_scores > threshold)[0]\n    \n    @staticmethod\n    def distance_matrix(points):\n        \"\"\"\n        Calculate distance matrix between points\n        Use case: Geographic distance, similarity\n        \n        Args:\n            points: List of (x, y) tuples or (lat, lon)\n        \"\"\"\n        points = np.array(points)\n        n = len(points)\n        distances = np.zeros((n, n))\n        \n        for i in range(n):\n            for j in range(n):\n                distances[i, j] = np.linalg.norm(points[i] - points[j])\n        \n        return distances\n    \n    @staticmethod\n    def angle_between_vectors(v1, v2):\n        \"\"\"\n        Calculate angle between two vectors\n        Use case: Shadow direction analysis, geolocation\n        \"\"\"\n        v1 = np.array(v1)\n        v2 = np.array(v2)\n        \n        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n        angle_rad = np.arccos(np.clip(cos_angle, -1, 1))\n        angle_deg = np.degrees(angle_rad)\n        \n        return angle_deg\n\n# Example usage:\ntoolkit = MathToolkit()\n\n# Basic stats\ndata = [1, 2, 3, 4, 5, 100]  # Note outlier\nstats = toolkit.basic_stats(data)\noutliers = toolkit.detect_outliers(data)\n\n# Geographic calculations\nlocations = [(52.2297, 21.0122), (51.5074, -0.1278)]  # Warsaw, London\ndistances = toolkit.distance_matrix(locations)\n```\n\n### **B. Pandas - Data Analysis**\n\n```python\n\"\"\"\nUse case: Tabular data, time series, aggregations\nBest for: Data manipulation, analysis\n\"\"\"\n\nimport pandas as pd\nfrom datetime import datetime\n\nclass DataAnalyzer:\n    \"\"\"Data analysis for Maya\"\"\"\n    \n    @staticmethod\n    def load_data(source, format='csv'):\n        \"\"\"Load data from various sources\"\"\"\n        if format == 'csv':\n            return pd.read_csv(source)\n        elif format == 'json':\n            return pd.read_json(source)\n        elif format == 'excel':\n            return pd.read_excel(source)\n        elif format == 'sql':\n            # Requires SQLAlchemy\n            from sqlalchemy import create_engine\n            engine = create_engine(source)\n            return pd.read_sql_table('table_name', engine)\n    \n    @staticmethod\n    def describe_data(df):\n        \"\"\"Get comprehensive description\"\"\"\n        return {\n            'shape': df.shape,\n            'columns': list(df.columns),\n            'dtypes': df.dtypes.to_dict(),\n            'missing': df.isnull().sum().to_dict(),\n            'stats': df.describe().to_dict(),\n            'memory': df.memory_usage(deep=True).sum()\n        }\n    \n    @staticmethod\n    def clean_data(df):\n        \"\"\"Basic data cleaning\"\"\"\n        # Remove duplicates\n        df = df.drop_duplicates()\n        \n        # Fill missing numeric with median\n        numeric_columns = df.select_dtypes(include=[np.number]).columns\n        for col in numeric_columns:\n            df[col].fillna(df[col].median(), inplace=True)\n        \n        # Fill missing categorical with mode\n        categorical_columns = df.select_dtypes(include=['object']).columns\n        for col in categorical_columns:\n            df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown', \n                          inplace=True)\n        \n        return df\n    \n    @staticmethod\n    def time_series_analysis(df, date_column, value_column):\n        \"\"\"Analyze time series data\"\"\"\n        df[date_column] = pd.to_datetime(df[date_column])\n        df = df.sort_values(date_column)\n        df.set_index(date_column, inplace=True)\n        \n        analysis = {\n            'daily_avg': df[value_column].resample('D').mean(),\n            'weekly_avg': df[value_column].resample('W').mean(),\n            'monthly_avg': df[value_column].resample('M').mean(),\n            'trend': df[value_column].rolling(window=7).mean(),\n            'total': df[value_column].sum(),\n            'date_range': (df.index.min(), df.index.max())\n        }\n        \n        return analysis\n    \n    @staticmethod\n    def group_analysis(df, group_by, agg_column):\n        \"\"\"Group and aggregate data\"\"\"\n        grouped = df.groupby(group_by)[agg_column].agg([\n            'count', 'sum', 'mean', 'median', 'std', 'min', 'max'\n        ])\n        return grouped\n    \n    @staticmethod\n    def pivot_analysis(df, index, columns, values):\n        \"\"\"Create pivot table\"\"\"\n        return pd.pivot_table(df, \n                             index=index,\n                             columns=columns,\n                             values=values,\n                             aggfunc='count',\n                             fill_value=0)\n    \n    @staticmethod\n    def correlation_matrix(df):\n        \"\"\"Calculate correlation matrix\"\"\"\n        numeric_df = df.select_dtypes(include=[np.number])\n        return numeric_df.corr()\n    \n    @staticmethod\n    def export_data(df, output_path, format='csv'):\n        \"\"\"Export data to various formats\"\"\"\n        if format == 'csv':\n            df.to_csv(output_path, index=False)\n        elif format == 'json':\n            df.to_json(output_path, orient='records', indent=2)\n        elif format == 'excel':\n            df.to_excel(output_path, index=False)\n        elif format == 'html':\n            df.to_html(output_path, index=False)\n\n# Example: Analyze Sejm data (from our previous project!)\nanalyzer = DataAnalyzer()\n\n# Load data\ndf = pd.read_json('sejm_analysis/sejm_asw_complete_analysis.json')\n\n# Describe\ndescription = analyzer.describe_data(df)\n\n# Time series analysis\nif 'date' in df.columns:\n    ts_analysis = analyzer.time_series_analysis(df, 'date', 'count')\n\n# Group by year\nif 'year' in df.columns:\n    yearly = analyzer.group_analysis(df, 'year', 'meeting_count')\n```\n\n### **C. SciPy - Scientific Computing**\n\n```python\n\"\"\"\nUse case: Statistical tests, optimization, signal processing\nBest for: Scientific analysis\n\"\"\"\n\nfrom scipy import stats\nfrom scipy.spatial import distance\nfrom scipy.optimize import minimize\n\nclass ScientificToolkit:\n    \"\"\"Advanced scientific computing\"\"\"\n    \n    @staticmethod\n    def statistical_test(group1, group2, test='ttest'):\n        \"\"\"\n        Compare two groups statistically\n        \n        Tests:\n        - ttest: T-test (parametric)\n        - mannwhitney: Mann-Whitney U (non-parametric)\n        - kstest: Kolmogorov-Smirnov\n        \"\"\"\n        if test == 'ttest':\n            statistic, pvalue = stats.ttest_ind(group1, group2)\n        elif test == 'mannwhitney':\n            statistic, pvalue = stats.mannwhitney(group1, group2)\n        elif test == 'kstest':\n            statistic, pvalue = stats.ks_2samp(group1, group2)\n        \n        return {\n            'statistic': statistic,\n            'pvalue': pvalue,\n            'significant': pvalue < 0.05,\n            'interpretation': 'Groups differ significantly' if pvalue < 0.05 else 'No significant difference'\n        }\n    \n    @staticmethod\n    def correlation_test(x, y, method='pearson'):\n        \"\"\"\n        Test correlation between variables\n        \n        Methods:\n        - pearson: Linear correlation\n        - spearman: Rank correlation\n        - kendalltau: Ordinal correlation\n        \"\"\"\n        if method == 'pearson':\n            corr, pvalue = stats.pearsonr(x, y)\n        elif method == 'spearman':\n            corr, pvalue = stats.spearmanr(x, y)\n        elif method == 'kendalltau':\n            corr, pvalue = stats.kendalltau(x, y)\n        \n        return {\n            'correlation': corr,\n            'pvalue': pvalue,\n            'significant': pvalue < 0.05,\n            'strength': 'strong' if abs(corr) > 0.7 else 'moderate' if abs(corr) > 0.4 else 'weak'\n        }\n    \n    @staticmethod\n    def calculate_distances(point, points, metric='euclidean'):\n        \"\"\"\n        Calculate distances from point to multiple points\n        \n        Use case: Find nearest locations, similarity\n        \n        Metrics: euclidean, manhattan, cosine, etc.\n        \"\"\"\n        distances = [distance.cdist([point], [p], metric=metric)[0][0] for p in points]\n        return distances\n    \n    @staticmethod\n    def optimize_function(func, initial_guess, bounds=None):\n        \"\"\"\n        Optimize (minimize) a function\n        \n        Use case: Find best parameters, locations\n        \"\"\"\n        result = minimize(func, initial_guess, bounds=bounds)\n        return {\n            'success': result.success,\n            'optimal_values': result.x,\n            'optimal_result': result.fun,\n            'iterations': result.nit\n        }\n\n# Example: Geographic optimization\ndef distance_to_multiple_points(location, target_locations):\n    \"\"\"Find location that minimizes sum of distances to targets\"\"\"\n    total_distance = sum([\n        distance.euclidean(location, target) \n        for target in target_locations\n    ])\n    return total_distance\n\n# Find optimal meeting point\ntargets = [(52.2297, 21.0122), (51.5074, -0.1278), (48.8566, 2.3522)]  # Warsaw, London, Paris\ninitial = (50.0, 10.0)  # Central Europe\n\ntoolkit = ScientificToolkit()\nresult = toolkit.optimize_function(\n    lambda loc: distance_to_multiple_points(loc, targets),\n    initial\n)\n# Result: Optimal meeting location\n```\n\n### **D. Scikit-learn - Machine Learning**\n\n```python\n\"\"\"\nUse case: Clustering, classification, anomaly detection\nBest for: Pattern recognition, predictions\n\"\"\"\n\nfrom sklearn.cluster import DBSCAN, KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nclass MLToolkit:\n    \"\"\"Machine learning for pattern detection\"\"\"\n    \n    @staticmethod\n    def cluster_data(data, method='kmeans', n_clusters=3):\n        \"\"\"\n        Cluster data points\n        \n        Use case: Group similar entities, detect communities\n        \"\"\"\n        # Normalize data\n        scaler = StandardScaler()\n        data_scaled = scaler.fit_transform(data)\n        \n        if method == 'kmeans':\n            model = KMeans(n_clusters=n_clusters, random_state=42)\n        elif method == 'dbscan':\n            model = DBSCAN(eps=0.5, min_samples=5)\n        \n        labels = model.fit_predict(data_scaled)\n        \n        return {\n            'labels': labels,\n            'n_clusters': len(set(labels)) - (1 if -1 in labels else 0),\n            'model': model\n        }\n    \n    @staticmethod\n    def detect_anomalies(data, contamination=0.1):\n        \"\"\"\n        Detect anomalies in data\n        \n        Use case: Find suspicious data points, outliers\n        \"\"\"\n        from sklearn.ensemble import IsolationForest\n        \n        model = IsolationForest(contamination=contamination, random_state=42)\n        predictions = model.fit_predict(data)\n        \n        # -1 = anomaly, 1 = normal\n        anomaly_indices = np.where(predictions == -1)[0]\n        \n        return {\n            'anomaly_indices': anomaly_indices,\n            'n_anomalies': len(anomaly_indices),\n            'anomaly_ratio': len(anomaly_indices) / len(data)\n        }\n    \n    @staticmethod\n    def reduce_dimensions(data, n_components=2):\n        \"\"\"\n        Reduce dimensionality for visualization\n        \n        Use case: Visualize high-dimensional data\n        \"\"\"\n        pca = PCA(n_components=n_components)\n        reduced = pca.fit_transform(data)\n        \n        return {\n            'reduced_data': reduced,\n            'explained_variance': pca.explained_variance_ratio_,\n            'total_variance_explained': sum(pca.explained_variance_ratio_)\n        }\n\n# Example: Cluster social media posts by similarity\n# (after converting to embeddings)\n```\n\n---\n\n## \ud83d\udcf8 3. IMAGE INTELLIGENCE TOOLKIT\n\n### **Tech Stack:**\n\n```python\n# requirements.txt\npillow==10.1.0            # Image processing\nopencv-python==4.8.1      # Computer vision\npytesseract==0.3.10       # OCR\nexiftool==0.12.0          # Metadata extraction\nimagehash==4.3.1          # Perceptual hashing\n```\n\n### **Implementation:**\n\n```python\n\"\"\"\nImage analysis for Elena\n\"\"\"\n\nfrom PIL import Image\nimport cv2\nimport pytesseract\nimport numpy as np\nimport imagehash\n\nclass ImageToolkit:\n    \"\"\"Image intelligence operations\"\"\"\n    \n    @staticmethod\n    def extract_exif(image_path):\n        \"\"\"Extract EXIF metadata\"\"\"\n        from PIL.ExifTags import TAGS\n        \n        image = Image.open(image_path)\n        exif_data = image._getexif()\n        \n        if not exif_data:\n            return {}\n        \n        metadata = {}\n        for tag_id, value in exif_data.items():\n            tag = TAGS.get(tag_id, tag_id)\n            metadata[tag] = value\n        \n        return metadata\n    \n    @staticmethod\n    def ocr_extract_text(image_path):\n        \"\"\"Extract text from image using OCR\"\"\"\n        image = Image.open(image_path)\n        text = pytesseract.image_to_string(image)\n        return text\n    \n    @staticmethod\n    def detect_faces(image_path):\n        \"\"\"Detect faces in image\"\"\"\n        # Load OpenCV face detector\n        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n        \n        # Read image\n        image = cv2.imread(image_path)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        \n        # Detect faces\n        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n        \n        return [{\n            'x': int(x),\n            'y': int(y),\n            'width': int(w),\n            'height': int(h)\n        } for (x, y, w, h) in faces]\n    \n    @staticmethod\n    def calculate_image_hash(image_path):\n        \"\"\"\n        Calculate perceptual hash\n        Use case: Find similar/duplicate images\n        \"\"\"\n        image = Image.open(image_path)\n        return {\n            'average_hash': str(imagehash.average_hash(image)),\n            'perceptual_hash': str(imagehash.phash(image)),\n            'difference_hash': str(imagehash.dhash(image))\n        }\n    \n    @staticmethod\n    def compare_images(image1_path, image2_path):\n        \"\"\"Compare two images for similarity\"\"\"\n        hash1 = imagehash.average_hash(Image.open(image1_path))\n        hash2 = imagehash.average_hash(Image.open(image2_path))\n        \n        difference = hash1 - hash2  # Hamming distance\n        \n        return {\n            'similarity_score': 1 - (difference / 64.0),  # 0-1 scale\n            'hamming_distance': difference,\n            'identical': difference == 0,\n            'similar': difference < 10  # Threshold\n        }\n    \n    @staticmethod\n    def analyze_colors(image_path):\n        \"\"\"Extract dominant colors\"\"\"\n        image = Image.open(image_path)\n        image = image.resize((150, 150))  # Reduce size for speed\n        \n        # Convert to numpy array\n        pixels = np.array(image)\n        pixels = pixels.reshape(-1, 3)\n        \n        # Use KMeans to find dominant colors\n        from sklearn.cluster import KMeans\n        kmeans = KMeans(n_clusters=5, random_state=42)\n        kmeans.fit(pixels)\n        \n        colors = kmeans.cluster_centers_.astype(int)\n        percentages = np.bincount(kmeans.labels_) / len(kmeans.labels_)\n        \n        return [\n            {\n                'rgb': tuple(color),\n                'hex': '#{:02x}{:02x}{:02x}'.format(*color),\n                'percentage': float(pct)\n            }\n            for color, pct in zip(colors, percentages)\n        ]\n```\n\n---\n\n## \ud83d\uddfa\ufe0f 4. GEOLOCATION TOOLKIT\n\n### **Tech Stack:**\n\n```python\n# requirements.txt\ngeopy==2.4.0              # Geocoding\nfolium==0.15.0            # Map visualization\npysolar==0.10             # Sun position calculation\ntimezonefinder==6.2.0     # Timezone from coordinates\n```\n\n### **Implementation:**\n\n```python\n\"\"\"\nGeolocation tools for Elena\n\"\"\"\n\nfrom geopy.geocoders import Nominatim\nfrom geopy.distance import geodesic\nimport folium\nfrom datetime import datetime\nfrom pysolar import solar\nfrom timezonefinder import TimezoneFinder\n\nclass GeolocationToolkit:\n    \"\"\"Geographic intelligence operations\"\"\"\n    \n    def __init__(self):\n        self.geolocator = Nominatim(user_agent=\"destiny-osint\")\n        self.tf = TimezoneFinder()\n    \n    def geocode(self, address):\n        \"\"\"Convert address to coordinates\"\"\"\n        location = self.geolocator.geocode(address)\n        if location:\n            return {\n                'address': location.address,\n                'latitude': location.latitude,\n                'longitude': location.longitude,\n                'raw': location.raw\n            }\n        return None\n    \n    def reverse_geocode(self, latitude, longitude):\n        \"\"\"Convert coordinates to address\"\"\"\n        location = self.geolocator.reverse(f\"{latitude}, {longitude}\")\n        if location:\n            return {\n                'address': location.address,\n                'raw': location.raw\n            }\n        return None\n    \n    def calculate_distance(self, coord1, coord2, unit='km'):\n        \"\"\"\n        Calculate distance between two coordinates\n        \n        Args:\n            coord1, coord2: (latitude, longitude) tuples\n            unit: 'km', 'miles', 'meters'\n        \"\"\"\n        distance_km = geodesic(coord1, coord2).kilometers\n        \n        if unit == 'km':\n            return distance_km\n        elif unit == 'miles':\n            return distance_km * 0.621371\n        elif unit == 'meters':\n            return distance_km * 1000\n    \n    def get_timezone(self, latitude, longitude):\n        \"\"\"Get timezone for coordinates\"\"\"\n        timezone = self.tf.timezone_at(lat=latitude, lng=longitude)\n        return timezone\n    \n    def calculate_sun_position(self, latitude, longitude, date_time):\n        \"\"\"\n        Calculate sun position for shadow analysis\n        \n        CRITICAL FOR CHRONOLOCATION!\n        \n        Returns:\n            azimuth: Direction of sun (0-360\u00b0, 0=North)\n            altitude: Height of sun above horizon (degrees)\n        \"\"\"\n        azimuth = solar.get_azimuth(latitude, longitude, date_time)\n        altitude = solar.get_altitude(latitude, longitude, date_time)\n        \n        return {\n            'azimuth': azimuth,\n            'altitude': altitude,\n            'datetime': date_time,\n            'coordinates': (latitude, longitude)\n        }\n    \n    def estimate_time_from_shadow(self, latitude, longitude, date, shadow_azimuth, tolerance=15):\n        \"\"\"\n        Estimate time of day from shadow direction\n        \n        BELLINGCAT TECHNIQUE!\n        \n        Args:\n            latitude, longitude: Location\n            date: Date (without time)\n            shadow_azimuth: Direction of shadow (degrees from North)\n            tolerance: Acceptable error in degrees\n        \n        Returns:\n            List of possible times\n        \"\"\"\n        possible_times = []\n        \n        # Try every 15 minutes throughout the day\n        for hour in range(24):\n            for minute in [0, 15, 30, 45]:\n                test_datetime = datetime(date.year, date.month, date.day, hour, minute)\n                \n                sun_pos = self.calculate_sun_position(latitude, longitude, test_datetime)\n                \n                # Shadow is opposite to sun\n                shadow_direction_from_sun = (sun_pos['azimuth'] + 180) % 360\n                \n                # Check if within tolerance\n                diff = abs(shadow_direction_from_sun - shadow_azimuth)\n                if diff > 180:\n                    diff = 360 - diff\n                \n                if diff <= tolerance:\n                    possible_times.append({\n                        'time': test_datetime.strftime('%H:%M'),\n                        'sun_azimuth': sun_pos['azimuth'],\n                        'sun_altitude': sun_pos['altitude'],\n                        'match_quality': (tolerance - diff) / tolerance\n                    })\n        \n        # Sort by match quality\n        possible_times.sort(key=lambda x: x['match_quality'], reverse=True)\n        \n        return possible_times\n    \n    def create_map(self, center, markers=None, output_file='map.html'):\n        \"\"\"\n        Create interactive map\n        \n        Args:\n            center: (latitude, longitude)\n            markers: List of {'coords': (lat, lon), 'popup': 'text', 'color': 'red'}\n        \"\"\"\n        map_obj = folium.Map(location=center, zoom_start=13)\n        \n        if markers:\n            for marker in markers:\n                folium.Marker(\n                    location=marker['coords'],\n                    popup=marker.get('popup', ''),\n                    icon=folium.Icon(color=marker.get('color', 'blue'))\n                ).add_to(map_obj)\n        \n        map_obj.save(output_file)\n        return output_file\n\n# Example: Shadow analysis (Bellingcat technique!)\ntoolkit = GeolocationToolkit()\n\n# Known: Location (Warsaw), Date (2024-11-04), Shadow direction (135\u00b0 from North)\nlocation = (52.2297, 21.0122)  # Warsaw\ndate = datetime(2024, 11, 4)\nshadow_azimuth = 135  # Shadow points Southeast\n\npossible_times = toolkit.estimate_time_from_shadow(\n    latitude=location[0],\n    longitude=location[1],\n    date=date,\n    shadow_azimuth=shadow_azimuth,\n    tolerance=15\n)\n\nprint(f\"Possible times: {possible_times[:3]}\")\n# Output: Most likely times when photo was taken!\n```\n\n---\n\n## \ud83d\udcca 5. COMPLETE AGENT TOOLKIT CLASS\n\n```python\n\"\"\"\nComplete toolkit dla ka\u017cdego agenta\nAll tools in one place\n\"\"\"\n\nclass AgentToolkit:\n    \"\"\"Complete toolkit for OSINT agents\"\"\"\n    \n    def __init__(self, agent_name):\n        self.agent_name = agent_name\n        \n        # Initialize all toolkits\n        self.scraper = BasicScraper()\n        self.dynamic_scraper = DynamicScraper()\n        self.math = MathToolkit()\n        self.data_analyzer = DataAnalyzer()\n        self.scientific = ScientificToolkit()\n        self.ml = MLToolkit()\n        self.image = ImageToolkit()\n        self.geo = GeolocationToolkit()\n    \n    def log(self, message):\n        \"\"\"Log message with agent name\"\"\"\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        print(f\"[{timestamp}] [{self.agent_name}] {message}\")\n    \n    # All methods accessible through self.scraper., self.math., etc.\n\n# Usage by Elena:\nelena_toolkit = AgentToolkit(\"Elena Volkov\")\n\n# Web scraping\nhtml = elena_toolkit.scraper.fetch_page('https://example.com')\n\n# Image analysis\nfaces = elena_toolkit.image.detect_faces('photo.jpg')\n\n# Geolocation\nlocation = elena_toolkit.geo.geocode(\"Warsaw, Poland\")\n\n# Math calculations\nstats = elena_toolkit.math.basic_stats([1, 2, 3, 4, 5])\n\n# Data analysis\ndf = elena_toolkit.data_analyzer.load_data('data.csv')\ndescription = elena_toolkit.data_analyzer.describe_data(df)\n```\n\n---\n\n## \ud83d\udce6 Installation Script\n\n```bash\n#!/bin/bash\n# install_toolkits.sh\n\necho \"Installing Agent Toolkits...\"\n\n# Core dependencies\npip install requests==2.31.0\npip install beautifulsoup4==4.12.2\npip install lxml==4.9.3\npip install playwright==1.40.0\npip install fake-useragent==1.4.0\n\n# Math & Data Science\npip install numpy==1.24.3\npip install scipy==1.11.3\npip install pandas==2.1.1\npip install scikit-learn==1.3.1\npip install statsmodels==0.14.0\n\n# Visualization\npip install matplotlib==3.8.0\npip install seaborn==0.12.2\npip install plotly==5.17.0\n\n# Image Processing\npip install pillow==10.1.0\npip install opencv-python==4.8.1\npip install pytesseract==0.3.10\npip install imagehash==4.3.1\n\n# Geolocation\npip install geopy==2.4.0\npip install folium==0.15.0\npip install pysolar==0.10\npip install timezonefinder==6.2.0\n\n# Playwright browser install\nplaywright install chromium\n\necho \"\u2705 All toolkits installed!\"\n```\n\n---\n\n## \ud83c\udfaf Next Steps\n\n1. \u2705 **Toolkits Defined** - Complete specification ready\n2. \ud83d\udd28 **Implementation** - Create Python modules\n3. \ud83d\udd28 **Testing** - Test each toolkit\n4. \ud83d\udd28 **Integration** - Connect with agents\n5. \ud83d\udd28 **Documentation** - Usage examples\n6. \ud83d\udd28 **Training** - Agents learn to use tools\n\n---\n\n**Prepared by:** Alex Morgan (Technical) + Elena Volkov (OSINT)  \n**Date:** 2025-11-04  \n**Status:** Ready for implementation  \n\n**All tools agents need to do world-class investigative work!** \ud83d\udee0\ufe0f\n",
  "indexed_at": "2025-11-04T18:49:17.348555",
  "source": "realtime_watcher"
}