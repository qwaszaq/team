{
  "file_path": "docs/concepts/DEEP_RESEARCH_TECHNICAL_ARCHITECTURE.md",
  "title": "\ud83c\udfd7\ufe0f Deep Research System - Technical Architecture",
  "document_type": "architecture",
  "content": "# \ud83c\udfd7\ufe0f Deep Research System - Technical Architecture\n\n**Document Type:** Technical Implementation Specification  \n**Date:** 2025-11-04  \n**Lead:** Maria Wi\u015bniewska (Software Architect)  \n**Contributors:** Tomasz, Piotr, Micha\u0142 (Core Team)\n\n---\n\n## \ud83d\udccb Technical Overview\n\nThis document provides detailed technical specifications for implementing the Deep Research Agent system.\n\n---\n\n## \ud83c\udfaf System Requirements\n\n### **Functional Requirements:**\n- FR1: Process 1M+ tokens of input data\n- FR2: Generate 50k token reports\n- FR3: Multi-agent coordination\n- FR4: Real-time progress tracking\n- FR5: Quality assurance gates\n- FR6: Source attribution\n\n### **Non-Functional Requirements:**\n- NFR1: Performance: Complete research in 5-8 hours\n- NFR2: Cost: < $70 per report\n- NFR3: Accuracy: 95%+ fact accuracy\n- NFR4: Scalability: Support 10 concurrent research tasks\n- NFR5: Reliability: 99% uptime\n- NFR6: Security: Enterprise-grade data protection\n\n---\n\n## \ud83c\udfd7\ufe0f Architecture Diagram\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        User Interface                           \u2502\n\u2502  (CLI / Web / API)                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Orchestration Layer                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Task Scheduler \u2502  \u2502 Agent Manager \u2502  \u2502 State Machine  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                               \u2502\n            \u25bc                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Agent Layer       \u2502          \u2502    Data Layer            \u2502\n\u2502                      \u2502          \u2502                          \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 Document Agent \u2502  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Vector Store (Qdrant)\u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 OSINT Agent    \u2502  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 PostgreSQL           \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 Financial      \u2502  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Redis Cache          \u2502 \u2502\n\u2502 \u2502 Analyst        \u2502  \u2502          \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502          \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502          \u2502 \u2502 S3 / File Storage    \u2502 \u2502\n\u2502 \u2502 Market Analyst \u2502  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502 \u2502 Strategic      \u2502  \u2502                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u2502 Analyst        \u2502  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 LLM Provider \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502                  \u2502  (Claude)    \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 \u2502 QC Agent       \u2502  \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 External APIs\u2502\n\u2502 \u2502 Synthesis      \u2502  \u2502                  \u2502 (Finance)    \u2502\n\u2502 \u2502 Agent          \u2502  \u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \ud83d\udcbe Data Architecture\n\n### **Storage Strategy:**\n\n#### **1. Vector Store (Qdrant)**\n```python\n# Use case: Semantic search over documents\ncollection_config = {\n    \"vectors\": {\n        \"size\": 3072,  # text-embedding-3-large\n        \"distance\": \"Cosine\"\n    },\n    \"payload_schema\": {\n        \"text\": \"text\",\n        \"source\": \"keyword\",\n        \"date\": \"datetime\",\n        \"chunk_id\": \"integer\",\n        \"doc_type\": \"keyword\"\n    }\n}\n\n# Example: Store document chunks\nqdrant.upsert(\n    collection_name=\"research_documents\",\n    points=[\n        {\n            \"id\": chunk_id,\n            \"vector\": embedding,\n            \"payload\": {\n                \"text\": chunk_text,\n                \"source\": \"Tesla_10K_2023.pdf\",\n                \"date\": \"2023-02-01\",\n                \"chunk_id\": i,\n                \"doc_type\": \"financial_report\"\n            }\n        }\n    ]\n)\n\n# Example: Semantic search\nresults = qdrant.search(\n    collection_name=\"research_documents\",\n    query_vector=query_embedding,\n    limit=50,\n    query_filter={\n        \"must\": [\n            {\"key\": \"date\", \"range\": {\"gte\": \"2020-01-01\"}},\n            {\"key\": \"doc_type\", \"match\": {\"value\": \"financial_report\"}}\n        ]\n    }\n)\n```\n\n#### **2. PostgreSQL (Structured Data)**\n```sql\n-- Schema design\nCREATE TABLE research_tasks (\n    task_id UUID PRIMARY KEY,\n    user_query TEXT NOT NULL,\n    status VARCHAR(50),  -- planning, collecting, analyzing, complete\n    created_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    cost_usd DECIMAL(10,2),\n    token_count INTEGER\n);\n\nCREATE TABLE agent_runs (\n    run_id UUID PRIMARY KEY,\n    task_id UUID REFERENCES research_tasks(task_id),\n    agent_type VARCHAR(50),  -- document, osint, financial, etc\n    status VARCHAR(50),\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    input_tokens INTEGER,\n    output_tokens INTEGER,\n    cost_usd DECIMAL(10,2),\n    result_summary TEXT\n);\n\nCREATE TABLE data_sources (\n    source_id UUID PRIMARY KEY,\n    task_id UUID REFERENCES research_tasks(task_id),\n    source_type VARCHAR(50),  -- pdf, api, web\n    source_url TEXT,\n    collected_at TIMESTAMP,\n    token_count INTEGER,\n    credibility_score DECIMAL(3,2)  -- 0.0 to 1.0\n);\n\nCREATE TABLE findings (\n    finding_id UUID PRIMARY KEY,\n    task_id UUID REFERENCES research_tasks(task_id),\n    agent_type VARCHAR(50),\n    finding_type VARCHAR(50),  -- metric, trend, red_flag, insight\n    content TEXT,\n    confidence_score DECIMAL(3,2),\n    sources TEXT[],  -- Array of source_ids\n    verified BOOLEAN DEFAULT FALSE\n);\n\nCREATE TABLE quality_checks (\n    check_id UUID PRIMARY KEY,\n    task_id UUID REFERENCES research_tasks(task_id),\n    check_type VARCHAR(50),  -- fact_check, consistency, bias\n    passed BOOLEAN,\n    issues TEXT,\n    checked_at TIMESTAMP\n);\n```\n\n#### **3. Redis Cache**\n```python\n# Use case: Cache expensive API calls and intermediate results\ncache_strategy = {\n    \"stock_prices\": ttl=86400,  # 24 hours\n    \"news_articles\": ttl=3600,  # 1 hour\n    \"financial_statements\": ttl=2592000,  # 30 days\n    \"embeddings\": ttl=604800,  # 7 days\n    \"agent_outputs\": ttl=3600  # 1 hour\n}\n\n# Example: Cache stock data\ndef get_stock_data(ticker, start_date, end_date):\n    cache_key = f\"stock:{ticker}:{start_date}:{end_date}\"\n    \n    # Check cache\n    cached = redis.get(cache_key)\n    if cached:\n        return json.loads(cached)\n    \n    # Fetch from API\n    data = yahoo_finance_api.get_history(ticker, start_date, end_date)\n    \n    # Cache for 24 hours\n    redis.setex(cache_key, 86400, json.dumps(data))\n    \n    return data\n```\n\n---\n\n## \ud83d\udd27 Agent Implementation\n\n### **Base Agent Class:**\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Any\nimport anthropic\n\nclass BaseResearchAgent(ABC):\n    def __init__(self, name: str, role: str):\n        self.name = name\n        self.role = role\n        self.llm = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n        self.vector_store = QdrantClient(...)\n        self.db = PostgreSQLConnection(...)\n        self.cache = RedisClient(...)\n    \n    @abstractmethod\n    def get_system_prompt(self) -> str:\n        \"\"\"Return agent-specific system prompt\"\"\"\n        pass\n    \n    @abstractmethod\n    def process(self, task: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Main processing logic\"\"\"\n        pass\n    \n    def call_llm(\n        self, \n        context: str, \n        query: str, \n        max_tokens: int = 4000\n    ) -> str:\n        \"\"\"Make LLM call with context management\"\"\"\n        \n        # Token count check\n        context_tokens = self.count_tokens(context)\n        \n        if context_tokens > 150000:\n            # Use RAG to reduce context\n            context = self.retrieve_relevant_context(query, max_tokens=100000)\n        \n        response = self.llm.messages.create(\n            model=\"claude-3-5-sonnet-20241022\",\n            max_tokens=max_tokens,\n            system=self.get_system_prompt(),\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"Context:\\n{context}\\n\\nQuery:\\n{query}\"\n            }]\n        )\n        \n        return response.content[0].text\n    \n    def retrieve_relevant_context(self, query: str, max_tokens: int) -> str:\n        \"\"\"RAG: Retrieve relevant chunks from vector store\"\"\"\n        \n        # Embed query\n        query_embedding = self.get_embedding(query)\n        \n        # Search vector store\n        results = self.vector_store.search(\n            collection_name=\"research_documents\",\n            query_vector=query_embedding,\n            limit=100\n        )\n        \n        # Collect chunks until token limit\n        context_chunks = []\n        total_tokens = 0\n        \n        for result in results:\n            chunk = result.payload[\"text\"]\n            chunk_tokens = self.count_tokens(chunk)\n            \n            if total_tokens + chunk_tokens <= max_tokens:\n                context_chunks.append(chunk)\n                total_tokens += chunk_tokens\n            else:\n                break\n        \n        return \"\\n\\n---\\n\\n\".join(context_chunks)\n    \n    def log_run(self, task_id: str, status: str, result: Dict):\n        \"\"\"Log agent run to database\"\"\"\n        self.db.execute(\"\"\"\n            INSERT INTO agent_runs (\n                task_id, agent_type, status, result_summary,\n                input_tokens, output_tokens, cost_usd\n            ) VALUES (%s, %s, %s, %s, %s, %s, %s)\n        \"\"\", (\n            task_id, self.role, status, json.dumps(result),\n            result.get(\"input_tokens\", 0),\n            result.get(\"output_tokens\", 0),\n            result.get(\"cost\", 0)\n        ))\n```\n\n### **Example: Financial Analyst Agent:**\n\n```python\nclass FinancialAnalystAgent(BaseResearchAgent):\n    def __init__(self):\n        super().__init__(name=\"FinancialAnalyst\", role=\"financial_analyst\")\n    \n    def get_system_prompt(self) -> str:\n        return \"\"\"You are an expert financial analyst with CFA certification.\n        \nYour role:\n- Analyze financial statements in depth\n- Calculate and interpret financial ratios\n- Identify trends, patterns, and anomalies\n- Detect red flags (accounting irregularities, unsustainable practices)\n- Provide evidence-based insights\n\nGuidelines:\n- Be precise with numbers\n- Cite specific financial statements\n- Show calculations\n- Consider industry context\n- Identify both strengths and weaknesses\n- Support conclusions with data\n\"\"\"\n    \n    def process(self, task: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Analyze financial data\"\"\"\n        \n        task_id = task[\"task_id\"]\n        company = task[\"company\"]\n        ticker = task[\"ticker\"]\n        years = task.get(\"years\", 10)\n        \n        # 1. Retrieve financial data\n        financial_data = self.retrieve_financial_data(ticker, years)\n        \n        # 2. Calculate metrics\n        metrics = self.calculate_metrics(financial_data)\n        \n        # 3. LLM analysis\n        analysis_prompt = f\"\"\"\nAnalyze the following financial data for {company} ({ticker}) \nover {years} years:\n\n{json.dumps(metrics, indent=2)}\n\nProvide comprehensive analysis covering:\n1. Profitability trends\n2. Liquidity position\n3. Solvency concerns\n4. Operational efficiency\n5. Growth trajectory\n6. Red flags or concerns\n7. Key insights\n\nFormat: Structured analysis with clear sections.\nLength: 8,000-10,000 tokens\n\"\"\"\n        \n        analysis = self.call_llm(\n            context=json.dumps(financial_data, indent=2),\n            query=analysis_prompt,\n            max_tokens=10000\n        )\n        \n        # 4. Store findings\n        self.store_findings(task_id, analysis, metrics)\n        \n        # 5. Log run\n        self.log_run(task_id, \"completed\", {\n            \"metrics_calculated\": len(metrics),\n            \"analysis_length\": len(analysis)\n        })\n        \n        return {\n            \"status\": \"completed\",\n            \"metrics\": metrics,\n            \"analysis\": analysis\n        }\n    \n    def calculate_metrics(self, financial_data: Dict) -> Dict:\n        \"\"\"Calculate financial ratios\"\"\"\n        metrics = {}\n        \n        for year, data in financial_data.items():\n            income = data[\"income_statement\"]\n            balance = data[\"balance_sheet\"]\n            cash_flow = data[\"cash_flow\"]\n            \n            metrics[year] = {\n                # Profitability\n                \"gross_margin\": income[\"gross_profit\"] / income[\"revenue\"],\n                \"operating_margin\": income[\"operating_income\"] / income[\"revenue\"],\n                \"net_margin\": income[\"net_income\"] / income[\"revenue\"],\n                \"roe\": income[\"net_income\"] / balance[\"shareholders_equity\"],\n                \"roa\": income[\"net_income\"] / balance[\"total_assets\"],\n                \n                # Liquidity\n                \"current_ratio\": balance[\"current_assets\"] / balance[\"current_liabilities\"],\n                \"quick_ratio\": (balance[\"current_assets\"] - balance[\"inventory\"]) / balance[\"current_liabilities\"],\n                \n                # Solvency\n                \"debt_to_equity\": balance[\"total_debt\"] / balance[\"shareholders_equity\"],\n                \"interest_coverage\": income[\"ebit\"] / income[\"interest_expense\"],\n                \n                # Efficiency\n                \"asset_turnover\": income[\"revenue\"] / balance[\"total_assets\"],\n                \"inventory_turnover\": income[\"cogs\"] / balance[\"inventory\"],\n                \n                # Cash flow\n                \"operating_cash_flow\": cash_flow[\"operating_cf\"],\n                \"free_cash_flow\": cash_flow[\"operating_cf\"] - cash_flow[\"capex\"]\n            }\n        \n        return metrics\n```\n\n---\n\n## \ud83d\udd04 Workflow Engine\n\n### **State Machine:**\n\n```python\nfrom enum import Enum\nfrom typing import Dict, List, Callable\n\nclass ResearchState(Enum):\n    CREATED = \"created\"\n    PLANNING = \"planning\"\n    COLLECTING = \"collecting\"\n    ANALYZING = \"analyzing\"\n    VERIFYING = \"verifying\"\n    SYNTHESIZING = \"synthesizing\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\nclass ResearchWorkflow:\n    def __init__(self, task_id: str, user_query: str):\n        self.task_id = task_id\n        self.user_query = user_query\n        self.state = ResearchState.CREATED\n        self.agents = self.initialize_agents()\n        self.results = {}\n    \n    def initialize_agents(self) -> Dict:\n        return {\n            \"orchestrator\": OrchestratorAgent(),\n            \"document\": DocumentIngestionAgent(),\n            \"osint\": OSINTAgent(),\n            \"financial\": FinancialAnalystAgent(),\n            \"market\": MarketAnalystAgent(),\n            \"strategic\": StrategicAnalystAgent(),\n            \"qc\": QualityControlAgent(),\n            \"synthesis\": SynthesisAgent()\n        }\n    \n    async def execute(self):\n        \"\"\"Execute research workflow\"\"\"\n        \n        try:\n            # Phase 1: Planning\n            self.transition_to(ResearchState.PLANNING)\n            plan = await self.agents[\"orchestrator\"].create_plan(self.user_query)\n            \n            # Phase 2: Data Collection\n            self.transition_to(ResearchState.COLLECTING)\n            await self.collect_data(plan)\n            \n            # Phase 3: Analysis (parallel)\n            self.transition_to(ResearchState.ANALYZING)\n            await self.run_analyses(plan)\n            \n            # Phase 4: Quality Control\n            self.transition_to(ResearchState.VERIFYING)\n            qc_result = await self.agents[\"qc\"].verify(self.results)\n            \n            if not qc_result[\"approved\"]:\n                # Re-run failed analyses\n                await self.rerun_failed_analyses(qc_result[\"issues\"])\n            \n            # Phase 5: Synthesis\n            self.transition_to(ResearchState.SYNTHESIZING)\n            report = await self.agents[\"synthesis\"].generate_report(self.results)\n            \n            # Complete\n            self.transition_to(ResearchState.COMPLETED)\n            return report\n            \n        except Exception as e:\n            self.transition_to(ResearchState.FAILED)\n            raise\n    \n    async def collect_data(self, plan: Dict):\n        \"\"\"Run data collection agents in parallel\"\"\"\n        tasks = [\n            self.agents[\"document\"].process(plan),\n            self.agents[\"osint\"].process(plan)\n        ]\n        results = await asyncio.gather(*tasks)\n        self.results[\"data_collection\"] = results\n    \n    async def run_analyses(self, plan: Dict):\n        \"\"\"Run analyst agents in parallel\"\"\"\n        tasks = [\n            self.agents[\"financial\"].process(plan),\n            self.agents[\"market\"].process(plan),\n            self.agents[\"strategic\"].process(plan)\n        ]\n        results = await asyncio.gather(*tasks)\n        self.results[\"analyses\"] = results\n    \n    def transition_to(self, new_state: ResearchState):\n        \"\"\"Transition to new state\"\"\"\n        print(f\"[{self.task_id}] {self.state.value} \u2192 {new_state.value}\")\n        self.state = new_state\n        \n        # Update database\n        self.db.execute(\"\"\"\n            UPDATE research_tasks \n            SET status = %s, updated_at = NOW()\n            WHERE task_id = %s\n        \"\"\", (new_state.value, self.task_id))\n```\n\n---\n\n## \ud83d\udea6 Rate Limiting & Throttling\n\n```python\nclass RateLimiter:\n    def __init__(self):\n        self.limits = {\n            \"anthropic\": {\n                \"requests_per_minute\": 50,\n                \"tokens_per_minute\": 40000\n            },\n            \"openai\": {\n                \"requests_per_minute\": 500,\n                \"tokens_per_minute\": 200000\n            },\n            \"alpha_vantage\": {\n                \"requests_per_minute\": 5\n            }\n        }\n        self.counters = {}\n    \n    async def acquire(self, service: str, tokens: int = 0):\n        \"\"\"Acquire rate limit token\"\"\"\n        if service not in self.counters:\n            self.counters[service] = {\"requests\": 0, \"tokens\": 0, \"reset_at\": time.time() + 60}\n        \n        counter = self.counters[service]\n        \n        # Reset if minute passed\n        if time.time() > counter[\"reset_at\"]:\n            counter[\"requests\"] = 0\n            counter[\"tokens\"] = 0\n            counter[\"reset_at\"] = time.time() + 60\n        \n        # Check limits\n        limits = self.limits[service]\n        if counter[\"requests\"] >= limits[\"requests_per_minute\"]:\n            wait_time = counter[\"reset_at\"] - time.time()\n            await asyncio.sleep(wait_time)\n            return await self.acquire(service, tokens)\n        \n        if tokens > 0 and counter[\"tokens\"] + tokens > limits.get(\"tokens_per_minute\", float(\"inf\")):\n            wait_time = counter[\"reset_at\"] - time.time()\n            await asyncio.sleep(wait_time)\n            return await self.acquire(service, tokens)\n        \n        # Acquire\n        counter[\"requests\"] += 1\n        counter[\"tokens\"] += tokens\n```\n\n---\n\n## \ud83d\udcb0 Cost Tracking\n\n```python\nclass CostTracker:\n    def __init__(self):\n        self.pricing = {\n            \"claude-3.5-sonnet\": {\n                \"input\": 0.003 / 1000,  # $0.003 per 1k tokens\n                \"output\": 0.015 / 1000\n            },\n            \"gpt-4-turbo\": {\n                \"input\": 0.01 / 1000,\n                \"output\": 0.03 / 1000\n            },\n            \"text-embedding-3-large\": {\n                \"input\": 0.00013 / 1000\n            }\n        }\n    \n    def calculate_llm_cost(\n        self, \n        model: str, \n        input_tokens: int, \n        output_tokens: int\n    ) -> float:\n        \"\"\"Calculate LLM API cost\"\"\"\n        pricing = self.pricing[model]\n        cost = (\n            input_tokens * pricing[\"input\"] +\n            output_tokens * pricing[\"output\"]\n        )\n        return cost\n    \n    def track_task_cost(self, task_id: str):\n        \"\"\"Get total cost for research task\"\"\"\n        result = self.db.execute(\"\"\"\n            SELECT \n                SUM(cost_usd) as total_cost,\n                SUM(input_tokens) as total_input_tokens,\n                SUM(output_tokens) as total_output_tokens\n            FROM agent_runs\n            WHERE task_id = %s\n        \"\"\", (task_id,))\n        \n        return result.fetchone()\n```\n\n---\n\n## \ud83d\udcca Monitoring & Observability\n\n```python\nimport prometheus_client as prom\n\n# Metrics\nresearch_duration = prom.Histogram(\n    \"research_duration_seconds\",\n    \"Time to complete research\",\n    [\"phase\"]\n)\n\nagent_duration = prom.Histogram(\n    \"agent_duration_seconds\",\n    \"Time per agent\",\n    [\"agent_type\"]\n)\n\nllm_tokens = prom.Counter(\n    \"llm_tokens_total\",\n    \"Total LLM tokens used\",\n    [\"model\", \"direction\"]  # direction: input/output\n)\n\nresearch_cost = prom.Counter(\n    \"research_cost_usd\",\n    \"Total cost in USD\",\n    [\"task_id\"]\n)\n\nquality_score = prom.Histogram(\n    \"quality_score\",\n    \"Research quality score\",\n    [\"task_id\"]\n)\n\n# Example usage in agent\n@research_duration.time()\ndef run_analysis_phase(self):\n    with agent_duration.labels(agent_type=\"financial\").time():\n        result = self.financial_agent.process()\n    \n    llm_tokens.labels(\n        model=\"claude-3.5-sonnet\",\n        direction=\"input\"\n    ).inc(result[\"input_tokens\"])\n    \n    llm_tokens.labels(\n        model=\"claude-3.5-sonnet\",\n        direction=\"output\"\n    ).inc(result[\"output_tokens\"])\n    \n    return result\n```\n\n---\n\n## \ud83d\udd12 Security Implementation\n\n### **API Key Management:**\n\n```python\nimport boto3\nfrom cryptography.fernet import Fernet\n\nclass SecretManager:\n    def __init__(self):\n        # Use AWS Secrets Manager or similar\n        self.client = boto3.client(\"secretsmanager\")\n        self.cache = {}\n    \n    def get_secret(self, secret_name: str) -> str:\n        \"\"\"Retrieve API key from secure vault\"\"\"\n        if secret_name in self.cache:\n            return self.cache[secret_name]\n        \n        response = self.client.get_secret_value(SecretId=secret_name)\n        secret = response[\"SecretString\"]\n        \n        # Cache for session\n        self.cache[secret_name] = secret\n        return secret\n\n# Usage\nsecrets = SecretManager()\nanthropic_key = secrets.get_secret(\"anthropic_api_key\")\n```\n\n### **Data Encryption:**\n\n```python\nclass DataEncryption:\n    def __init__(self, key: bytes):\n        self.cipher = Fernet(key)\n    \n    def encrypt_findings(self, findings: Dict) -> bytes:\n        \"\"\"Encrypt sensitive research findings\"\"\"\n        json_data = json.dumps(findings)\n        encrypted = self.cipher.encrypt(json_data.encode())\n        return encrypted\n    \n    def decrypt_findings(self, encrypted_data: bytes) -> Dict:\n        \"\"\"Decrypt findings\"\"\"\n        decrypted = self.cipher.decrypt(encrypted_data)\n        return json.loads(decrypted.decode())\n```\n\n---\n\n## \ud83d\ude80 Deployment Architecture\n\n### **Option 1: Local Development**\n```yaml\n# docker-compose.yml\nversion: \"3.8\"\n\nservices:\n  qdrant:\n    image: qdrant/qdrant:latest\n    ports:\n      - \"6333:6333\"\n    volumes:\n      - qdrant_data:/qdrant/storage\n  \n  postgres:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: research_db\n      POSTGRES_USER: research_user\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n  \n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n  \n  research_api:\n    build: .\n    depends_on:\n      - qdrant\n      - postgres\n      - redis\n    environment:\n      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}\n    ports:\n      - \"8000:8000\"\n\nvolumes:\n  qdrant_data:\n  postgres_data:\n```\n\n### **Option 2: Production (AWS)**\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Route 53 (DNS)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      CloudFront (CDN)                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Application Load Balancer            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 ECS     \u2502    \u2502 ECS     \u2502\n    \u2502 Task 1  \u2502    \u2502 Task 2  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n    \u2502     RDS PostgreSQL      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   ElastiCache Redis     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   Qdrant Cloud          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   S3 (Documents)        \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n---\n\n## \u2705 Implementation Checklist\n\n### **Phase 1: MVP (Weeks 1-4)**\n- [ ] Set up infrastructure (Docker Compose)\n- [ ] Implement base agent class\n- [ ] Build document ingestion agent\n- [ ] Build financial analyst agent\n- [ ] Build synthesis agent\n- [ ] Create basic workflow\n- [ ] Test on sample company\n\n### **Phase 2: Core Agents (Weeks 5-8)**\n- [ ] Build OSINT agent\n- [ ] Build market analyst agent\n- [ ] Build strategic analyst agent\n- [ ] Build quality control agent\n- [ ] Integrate all agents\n- [ ] Add parallel processing\n\n### **Phase 3: Production (Weeks 9-12)**\n- [ ] Web UI development\n- [ ] API endpoints\n- [ ] Authentication & authorization\n- [ ] Cost optimization\n- [ ] Performance tuning\n- [ ] Documentation\n- [ ] Deployment to production\n\n---\n\n## \ud83d\udcc8 Success Metrics\n\n**Target KPIs:**\n- Research completion time: < 8 hours\n- Cost per report: < $70\n- Fact accuracy: > 95%\n- User satisfaction: > 4.5/5\n- System uptime: > 99%\n\n---\n\n## \ud83c\udfaf Conclusion\n\nThis architecture provides:\n- \u2705 Scalable multi-agent system\n- \u2705 Efficient token management\n- \u2705 Cost-effective processing\n- \u2705 High-quality outputs\n- \u2705 Production-ready design\n\n**Status:** Ready for implementation\n\n---\n\n**Document Version:** 1.0  \n**Last Updated:** 2025-11-04  \n**Next Review:** After MVP completion\n",
  "indexed_at": "2025-11-04T14:57:54.434217",
  "source": "realtime_watcher"
}