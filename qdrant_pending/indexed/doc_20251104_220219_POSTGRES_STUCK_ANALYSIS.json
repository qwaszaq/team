{
  "file_path": "POSTGRES_STUCK_ANALYSIS.md",
  "title": "PostgreSQL Stuck Analysis - November 4, 2025",
  "document_type": "analysis",
  "content": "# PostgreSQL Stuck Analysis - November 4, 2025\n\n## Summary of Issue\nThe PostgreSQL command `\\d es_document_references` was stuck for 431+ seconds. This appears to be caused by multiple concurrent database operations and complex schema design.\n\n## Root Causes Identified\n\n### 1. Heavy Database Schema Complexity\nThe `es_document_references` table has:\n- **6 indexes** including 3 GIN indexes (for arrays and JSONB)\n- **Foreign key constraints**\n- **Materialized view** that joins with usage logs\n- **Multiple PL/pgSQL functions**\n\nGIN indexes are particularly expensive to maintain during inserts/updates.\n\n### 2. Massive Concurrent Write Operations\n- **180+ SQL files** in `sql/realtime_updates/` directory (712KB total)\n- **2,022 lines of SQL updates** being processed\n- Multiple automated scripts running simultaneously:\n  - `helena_realtime_processor.py` - Processes .md files and writes to 4 databases\n  - Morning brief automation\n  - Various sync scripts\n\n### 3. Real-time Processing Architecture\nThe system has real-time watchers that:\n- Monitor file changes\n- Immediately process and insert into PostgreSQL\n- Generate SQL update files for each operation\n- Update multiple tables with complex JSONB data\n\n### 4. Database Connection Saturation\n- Multiple scripts connecting simultaneously\n- Each script potentially holding locks during complex operations\n- GIN index updates can block other operations\n\n## Why the Command Got Stuck\n\nWhen you ran `\\d es_document_references`, PostgreSQL needed to:\n1. Acquire metadata locks to read table structure\n2. Query system catalogs for all indexes, constraints, and views\n3. Wait for any active transactions modifying the table structure\n\nWith heavy concurrent writes and GIN index updates, the metadata query was blocked waiting for locks.\n\n## Recommendations\n\n### Immediate Actions\n1. **Check for blocking queries**:\n   ```sql\n   SELECT pid, usename, query, state, wait_event_type, wait_event \n   FROM pg_stat_activity \n   WHERE datname = 'destiny_team' AND state != 'idle';\n   ```\n\n2. **Kill blocking connections if needed**:\n   ```sql\n   SELECT pg_terminate_backend(pid) \n   FROM pg_stat_activity \n   WHERE datname = 'destiny_team' \n   AND state = 'active' \n   AND query_start < NOW() - interval '5 minutes';\n   ```\n\n### Long-term Solutions\n1. **Optimize GIN Indexes**: Consider using `gin_pending_list_limit` to batch GIN updates\n2. **Batch Processing**: Instead of real-time inserts, batch them every few seconds\n3. **Connection Pooling**: Use pgbouncer to limit concurrent connections\n4. **Separate Read/Write Workloads**: Use read replicas for queries\n5. **Monitor Table Bloat**: GIN indexes can cause significant bloat\n\n## Current System State\n- PostgreSQL is running in Docker container\n- Multiple automated processes writing concurrently\n- Heavy use of JSONB and array operations\n- Real-time processing creating continuous write load\n\nThe system is experiencing contention between:\n- Real-time document processing\n- Index maintenance overhead\n- Metadata queries\n- Concurrent write operations",
  "indexed_at": "2025-11-04T22:02:19.825439",
  "source": "realtime_watcher"
}